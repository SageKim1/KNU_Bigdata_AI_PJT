{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370455e-9a9e-4093-ac07-aae7b1517f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac142a8c-4dd0-485d-be7b-20670e5b2f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d085d48-ec55-41dd-a3cb-0c12979edbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Train Embedding Ï§ë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|                                                                        | 0/638 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 6 but got size 13 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 155\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# ÏûÑÎ≤†Îî© Ï∂îÏ∂ú\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîπ Train Embedding Ï§ë...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m train_embeddings = \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîπ Validation Embedding Ï§ë...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m val_embeddings = get_embeddings(clip_model, val_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(model, loader)\u001b[39m\n\u001b[32m     73\u001b[39m all_embeddings = []\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEmbedding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpixel_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Project\\PJT_10\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Project\\PJT_10\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Project\\PJT_10\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Project\\PJT_10\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mcollate_fn\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollate_fn\u001b[39m(batch):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     input_ids = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     attention_mask = torch.cat([item[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch], dim=\u001b[32m0\u001b[39m)\n\u001b[32m     59\u001b[39m     pixel_values = torch.cat([item[\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch], dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 0. Expected size 6 but got size 13 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "import os, torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# ÏÑ§Ï†ï\n",
    "DATA_DIR = r\"D:\\Project\\PJT_10\\shopee-product-matching\"\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "IMG_DIR = os.path.join(DATA_DIR, \"train_images\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎî© Î∞è Ïù∏ÏΩîÎî©\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"label_group\"] = df[\"label_group\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Group Í∏∞Î∞ò Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\n",
    "train_idx, temp_idx = next(gss.split(df, groups=df[\"label_group\"]))\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "temp_df = df.iloc[temp_idx].reset_index(drop=True)\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_idx, test_idx = next(gss2.split(temp_df, groups=temp_df[\"label_group\"]))\n",
    "val_df = temp_df.iloc[val_idx].reset_index(drop=True)\n",
    "test_df = temp_df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "# Dataset Ï†ïÏùò\n",
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, processor):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(os.path.join(self.img_dir, row[\"image\"])).convert(\"RGB\")\n",
    "        text = row[\"title\"]\n",
    "        return self.processor(text=[text], images=[image], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Collate Ìï®Ïàò\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.cat([item[\"input_ids\"] for item in batch], dim=0)\n",
    "    attention_mask = torch.cat([item[\"attention_mask\"] for item in batch], dim=0)\n",
    "    pixel_values = torch.cat([item[\"pixel_values\"] for item in batch], dim=0)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"pixel_values\": pixel_values\n",
    "    }\n",
    "\n",
    "# Î™®Îç∏ Î∞è Processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Ìï®Ïàò\n",
    "def get_embeddings(model, loader):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Embedding\"):\n",
    "            inputs = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            image_embeds = model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "            text_embeds = model.get_text_features(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "            combined = torch.cat([image_embeds, text_embeds], dim=1)\n",
    "            all_embeddings.append(combined.cpu())\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(ShopeeDataset(train_df, IMG_DIR, clip_processor), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ShopeeDataset(val_df, IMG_DIR, clip_processor), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(ShopeeDataset(test_df, IMG_DIR, clip_processor), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# F1 & Ï†ïÌôïÎèÑ ÌèâÍ∞Ä Ìï®Ïàò\n",
    "def compute_row_wise_metrics(embeds, df, top_k=5):\n",
    "    sim_matrix = cosine_similarity(embeds)\n",
    "    np.fill_diagonal(sim_matrix, -1)  # ÏûêÍ∏∞ ÏûêÏã† Ï†úÏô∏\n",
    "\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        true_ids = df[df[\"label_group\"] == row[\"label_group\"]][\"posting_id\"].tolist()\n",
    "        top_idx = np.argsort(sim_matrix[i])[::-1][:top_k]\n",
    "        pred_ids = df.iloc[top_idx][\"posting_id\"].tolist()\n",
    "        preds.append(set(pred_ids))\n",
    "        targets.append(set(true_ids))\n",
    "\n",
    "    # F1 Score (macro ÌèâÍ∑†)\n",
    "    row_f1 = [f1_score(list(t), list(p), average='macro') for p, t in zip(preds, targets)]\n",
    "\n",
    "    # Accuracy: Ï†ïÌôïÌûà ÏòàÏ∏°ÌñàÎäîÍ∞Ä (set Í∏∞Ï§Ä ÏôÑÏ†Ñ ÏùºÏπò)\n",
    "    row_acc = [1.0 if p == t else 0.0 for p, t in zip(preds, targets)]\n",
    "\n",
    "    return np.mean(row_f1), np.mean(row_acc)\n",
    "\n",
    "\n",
    "def compute_loss_cosine(embeds, df, top_k=5):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    sim_matrix = cosine_similarity(embeds)\n",
    "    np.fill_diagonal(sim_matrix, -1)\n",
    "\n",
    "    # Ï†ïÎãµ Î≤°ÌÑ∞\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    posting_ids = df[\"posting_id\"].tolist()\n",
    "    pid_to_label = dict(zip(df[\"posting_id\"], df[\"label_group\"]))\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        anchor_label = row[\"label_group\"]\n",
    "        anchor_id = row[\"posting_id\"]\n",
    "\n",
    "        # ÏÉÅÏúÑ top_k ÌõÑÎ≥¥ ÏÑ†ÌÉù\n",
    "        sim_row = sim_matrix[i]\n",
    "        top_indices = np.argsort(sim_row)[::-1][:top_k]\n",
    "        for j in top_indices:\n",
    "            candidate_id = df.iloc[j][\"posting_id\"]\n",
    "            candidate_label = df.iloc[j][\"label_group\"]\n",
    "            similarity = sim_matrix[i][j]\n",
    "\n",
    "            y_true.append(1.0 if anchor_label == candidate_label else 0.0)\n",
    "            y_pred.append(similarity)\n",
    "\n",
    "    # Normalize similarity to [0, 1] if needed\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = (y_pred - y_pred.min()) / (y_pred.max() - y_pred.min() + 1e-8)\n",
    "\n",
    "    y_true_tensor = torch.tensor(y_true, dtype=torch.float32)\n",
    "    y_pred_tensor = torch.tensor(y_pred, dtype=torch.float32)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    loss = criterion(y_pred_tensor, y_true_tensor)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Ï∂îÏ∂ú\n",
    "print(\"üîπ Train Embedding Ï§ë...\")\n",
    "train_embeddings = get_embeddings(clip_model, train_loader)\n",
    "\n",
    "print(\"üîπ Validation Embedding Ï§ë...\")\n",
    "val_embeddings = get_embeddings(clip_model, val_loader)\n",
    "\n",
    "print(\"üîπ Test Embedding Ï§ë...\")\n",
    "test_embeddings = get_embeddings(clip_model, test_loader)\n",
    "\n",
    "# ÌèâÍ∞Ä\n",
    "print(\"\\nüìä [Train Metrics]\")\n",
    "train_f1, train_acc = compute_row_wise_metrics(train_embeddings, train_df)\n",
    "train_loss = compute_loss_cosine(train_embeddings, train_df)\n",
    "print(f\"Mean F1 (train): {train_f1:.4f}\")\n",
    "print(f\"Mean Accuracy (train): {train_acc:.4f}\")\n",
    "print(f\"Loss (train): {train_loss:.4f}\")\n",
    "\n",
    "print(\"\\nüìä [Val Metrics]\")\n",
    "val_f1, val_acc = compute_row_wise_metrics(val_embeddings, val_df)\n",
    "val_loss = compute_loss_cosine(val_embeddings, val_df)\n",
    "print(f\"Mean F1 (val): {val_f1:.4f}\")\n",
    "print(f\"Mean Accuracy (val): {val_acc:.4f}\")\n",
    "print(f\"Loss (val): {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nüìä [Test Metrics]\")\n",
    "test_f1, test_acc = compute_row_wise_metrics(test_embeddings, test_df)\n",
    "test_loss = compute_loss_cosine(test_embeddings, test_df)\n",
    "print(f\"Mean F1 (test): {test_f1:.4f}\")\n",
    "print(f\"Mean Accuracy (test): {test_acc:.4f}\")\n",
    "print(f\"Loss (test): {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8706f6-75b5-4eb9-9c73-c0a591166658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5b398-3228-4efe-a74a-a5c3b07da647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce27b3-693a-4907-9b7e-f022d4440ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
