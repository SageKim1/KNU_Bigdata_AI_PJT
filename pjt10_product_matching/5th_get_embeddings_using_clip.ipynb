{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b12bf-4659-44ff-ae97-ecc6ed41ee15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4964a311-b6e9-452b-a5da-d178dea526f1",
   "metadata": {},
   "source": [
    "# ÏûÑÎ≤†Îî©ÏùÑ NPY, CSVÎ°ú Ï†ÄÏû• -> Ïûò Îê®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe0aa16-8dd5-4de0-9df4-2c7fa6b05456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
    "import os, random, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# 1. ÏÑ§Ï†ï\n",
    "DATA_DIR = r\"D:\\Project\\PJT_10\\shopee-product-matching\"\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "IMG_DIR = os.path.join(DATA_DIR, \"train_images\")\n",
    "\n",
    "SAVE_DIR = \"./saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ Î°úÎî© Î∞è ÎùºÎ≤® Ïù∏ÏΩîÎî©\n",
    "df = pd.read_csv(CSV_PATH).reset_index(drop=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label_encoded\"] = label_encoder.fit_transform(df[\"label_group\"])\n",
    "\n",
    "# 3. Í∑∏Î£π Îã®ÏúÑÎ°ú train/val/test split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.4, random_state=SEED)\n",
    "train_idx, temp_idx = next(gss.split(df, groups=df[\"label_encoded\"]))\n",
    "\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "temp_df = df.iloc[temp_idx].reset_index(drop=True)\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=SEED)\n",
    "val_idx, test_idx = next(gss2.split(temp_df, groups=temp_df[\"label_encoded\"]))\n",
    "\n",
    "val_df = temp_df.iloc[val_idx].reset_index(drop=True)\n",
    "test_df = temp_df.iloc[test_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad33063e-182e-4689-a43a-7bcb848707fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20392 6820 7038\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2bf5813-583a-45cb-b0d2-590bff0b8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 7. Î™®Îç∏ Ï†ïÏùò (Siamese)\n",
    "class CLIPSiameseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.clip.config.projection_dim * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, pixel_values1,\n",
    "                      input_ids2, attention_mask2, pixel_values2):\n",
    "        text_features1 = self.clip.get_text_features(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "        image_features1 = self.clip.get_image_features(pixel_values=pixel_values1)\n",
    "        feat1 = torch.cat([image_features1, text_features1], dim=1)\n",
    "\n",
    "        text_features2 = self.clip.get_text_features(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "        image_features2 = self.clip.get_image_features(pixel_values=pixel_values2)\n",
    "        feat2 = torch.cat([image_features2, text_features2], dim=1)\n",
    "\n",
    "        combined = torch.cat([feat1, feat2], dim=1)\n",
    "        output = self.classifier(combined).squeeze(1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6438f637-beef-4a8a-afd1-ef2846045ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model from ./saved_models\\clip_pair_best_epoch2_20250716_025548.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7038/7038 [03:09<00:00, 37.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved test embeddings to ./saved_models\\test_embeddings.npy\n",
      "üÜî Saved posting IDs to ./saved_models\\test_posting_ids.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "SAVE_DIR = \"./saved_models\"\n",
    "# Ï†ÄÏû•Ìï† ÏúÑÏπò\n",
    "EMBED_SAVE_PATH = os.path.join(SAVE_DIR, \"test_embeddings.npy\")\n",
    "ID_SAVE_PATH = os.path.join(SAVE_DIR, \"test_posting_ids.npy\")\n",
    "\n",
    "# 1. Î™®Îç∏ Ï¥àÍ∏∞Ìôî Î∞è Í∞ÄÏ§ëÏπò Î∂àÎü¨Ïò§Í∏∞\n",
    "model = CLIPSiameseModel(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "best_model_path = os.path.join(SAVE_DIR, \"clip_pair_best_epoch2_20250716_025548.pth\")\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "print(f\"‚úÖ Loaded model from {best_model_path}\")\n",
    "\n",
    "# 2. processor Î°úÎìú\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# 3. ÌÖåÏä§Ìä∏ÏÖã Îã®Ïùº ÏûÖÎ†• Í∏∞Î∞ò ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
    "embeddings = []\n",
    "posting_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Generating embeddings\"):\n",
    "        image_path = os.path.join(IMG_DIR, row[\"image\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        text = row[\"title\"]\n",
    "\n",
    "        inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "        text_feat = model.clip.get_text_features(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        image_feat = model.clip.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "\n",
    "        combined_feat = torch.cat([image_feat, text_feat], dim=1)  # shape: [1, 1024]\n",
    "        embeddings.append(combined_feat.squeeze(0).cpu().numpy())\n",
    "        posting_ids.append(row[\"posting_id\"])\n",
    "\n",
    "# 4. numpyÎ°ú Ï†ÄÏû•\n",
    "embeddings = np.stack(embeddings)  # shape: [N, 1024]\n",
    "np.save(EMBED_SAVE_PATH, embeddings)\n",
    "np.save(ID_SAVE_PATH, np.array(posting_ids))\n",
    "\n",
    "print(f\"üíæ Saved test embeddings to {EMBED_SAVE_PATH}\")\n",
    "print(f\"üÜî Saved posting IDs to {ID_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a8a51-a163-4abd-ae6b-151e83e0f4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27f682c2-f6f6-4f7d-96d5-e4c4c12b7346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ Saved embeddings to CSV at ./saved_models\\test_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# csvÎ°ú Ï†ÄÏû•\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_embed = pd.DataFrame(embeddings)\n",
    "df_embed[\"posting_id\"] = posting_ids\n",
    "csv_path = os.path.join(SAVE_DIR, \"test_embeddings.csv\")\n",
    "df_embed.to_csv(csv_path, index=False)\n",
    "print(f\"üßæ Saved embeddings to CSV at {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076059b-556d-4796-892b-9706e4d62ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d59e8c-d8cd-4eef-9470-11a6ce226d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfe7e5-7c80-4248-84b2-7daea8dd283e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ea24c-831a-4376-9008-3d34cc6115c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36002f1c-e5bf-413d-aa01-33fd9f207327",
   "metadata": {},
   "source": [
    "# faiss, ANN -> ÌòÑÏû¨ Ïò§Î•ò ÎÇòÏûàÎäî ÏÉÅÌÉú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d567ab1-0a5f-4718-b309-88add79138c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ‚úÖ 1. Faiss Í∏∞Î∞ò ÏµúÍ∑ºÏ†ë Í≤ÄÏÉâ ÏΩîÎìú\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# üîß ÏÑ§Ï†ï: ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∞è ÌååÏùº Î°úÎî©\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ÏûÑÎ≤†Îî© Î∞è ID Î°úÎî©\u001b[39;00m\n\u001b[32m      8\u001b[39m embedding_path = \u001b[33m\"\u001b[39m\u001b[33m./saved_models/test_embeddings.npy\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "# ‚úÖ 1. Faiss Í∏∞Î∞ò ÏµúÍ∑ºÏ†ë Í≤ÄÏÉâ ÏΩîÎìú\n",
    "# üîß ÏÑ§Ï†ï: ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∞è ÌååÏùº Î°úÎî©\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Î∞è ID Î°úÎî©\n",
    "embedding_path = \"./saved_models/test_embeddings.npy\"\n",
    "id_path = \"./saved_models/test_posting_ids.npy\"\n",
    "\n",
    "embeddings = np.load(embedding_path).astype(\"float32\")  # (N, 1024)\n",
    "posting_ids = np.load(id_path)  # (N,)\n",
    "\n",
    "print(f\"‚úÖ Embedding shape: {embeddings.shape}, IDs: {len(posting_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf013dc-e532-485a-ae7b-ccebadf5bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 2. Faiss Ïù∏Îç±Ïä§ Íµ¨Ï∂ï\n",
    "\n",
    "# L2 Í±∞Î¶¨ Í∏∞Î∞ò Ïù∏Îç±Ïä§ ÏÉùÏÑ±\n",
    "dim = embeddings.shape[1]  # Î≥¥ÌÜµ 1024\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# ÏûÑÎ≤†Îî© Ï∂îÍ∞Ä\n",
    "index.add(embeddings)\n",
    "print(f\"‚úÖ Index populated with {index.ntotal} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91e65f-6ae0-4ca0-860b-46a12644d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 3. ÏµúÍ∑ºÏ†ë Ïù¥ÏõÉ Í≤ÄÏÉâ (Top-K)\n",
    "\n",
    "# ÏøºÎ¶¨: ÌÖåÏä§Ìä∏ÏÖãÏùò Ï≤´ Î≤àÏß∏ ÏûÑÎ≤†Îî©\n",
    "query_vec = embeddings[0].reshape(1, -1)  # (1, 1024)\n",
    "\n",
    "# KÍ∞ú ÏµúÍ∑ºÏ†ë Ïù¥ÏõÉ Í≤ÄÏÉâ (ÏûêÍ∏∞ ÏûêÏã† Ìè¨Ìï®Îê®)\n",
    "k = 5\n",
    "distances, indices = index.search(query_vec, k)\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(\"\\nüîç Top-K Nearest Neighbors:\")\n",
    "for rank, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
    "    print(f\"{rank+1}. ID: {posting_ids[idx]}, Distance: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ee6201-4696-4cfb-b481-94b766c25794",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ‚úÖ 4. (ÏÑ†ÌÉù) Ï†ÑÏ≤¥ ÏÉÅÌíàÏóê ÎåÄÌï¥ Top-K ÏµúÍ∑ºÏ†ë Ïù¥ÏõÉ Ï†ÄÏû•\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Ï†ÑÏ≤¥ test_df Í∏∞Ï§ÄÏúºÎ°ú top-K Ïú†ÏÇ¨ posting_id ÎΩëÍ∏∞\u001b[39;00m\n\u001b[32m      4\u001b[39m k = \u001b[32m5\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m all_distances, all_indices = \u001b[43mindex\u001b[49m.search(embeddings, k)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Í≤∞Í≥º Ï†ÄÏû• (Î¶¨Ïä§Ìä∏ ÌòïÌÉú)\u001b[39;00m\n\u001b[32m      8\u001b[39m results = []\n",
      "\u001b[31mNameError\u001b[39m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "# ‚úÖ 4. (ÏÑ†ÌÉù) Ï†ÑÏ≤¥ ÏÉÅÌíàÏóê ÎåÄÌï¥ Top-K ÏµúÍ∑ºÏ†ë Ïù¥ÏõÉ Ï†ÄÏû•\n",
    "\n",
    "# Ï†ÑÏ≤¥ test_df Í∏∞Ï§ÄÏúºÎ°ú top-K Ïú†ÏÇ¨ posting_id ÎΩëÍ∏∞\n",
    "k = 5\n",
    "all_distances, all_indices = index.search(embeddings, k)\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû• (Î¶¨Ïä§Ìä∏ ÌòïÌÉú)\n",
    "results = []\n",
    "for i, neighbors in enumerate(all_indices):\n",
    "    anchor_id = posting_ids[i]\n",
    "    neighbor_ids = [posting_ids[n] for n in neighbors if posting_ids[n] != anchor_id]\n",
    "    results.append({\"posting_id\": anchor_id, \"matches\": \" \".join(neighbor_ids)})\n",
    "\n",
    "# pandasÎ°ú Ï†ÄÏû•\n",
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(\"./saved_models/faiss_topk_result.csv\", index=False)\n",
    "print(\"üíæ Saved top-K retrieval result to ./saved_models/faiss_topk_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9672f7-cf79-4e5b-81c8-0c2fec6aa831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023f238-6438-4368-a4f2-2fbecbfa5ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83ae94-964f-45f5-8c34-2fda554009d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded01ce-69b4-47db-809c-9dcac53f7e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
