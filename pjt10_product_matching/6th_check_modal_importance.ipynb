{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fcc2aa-02c7-45fc-871a-bd7201168006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Modality별 Weight 영향도 (1st Linear Layer 기준) ===\n",
      "Image1 Weight Norm: 0.2881\n",
      "Text1  Weight Norm: 0.2892\n",
      "Image2 Weight Norm: 0.2886\n",
      "Text2  Weight Norm: 0.2886\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 예시 구조 (모델 정의 예시)\n",
    "class SimpleClassifier(torch.nn.Module):\n",
    "    def __init__(self, proj_dim=512):\n",
    "        super().__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(proj_dim * 4, 512),  # (img1, txt1, img2, txt2)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "# 모델 인스턴스 생성 및 첫 번째 레이어 weight 추출\n",
    "proj_dim = 512\n",
    "model = SimpleClassifier(proj_dim=proj_dim)\n",
    "first_linear_layer = model.classifier[0]\n",
    "weight = first_linear_layer.weight.data.cpu().numpy()  # (512, 2048)\n",
    "\n",
    "# 구간별로 나누기: image1, text1, image2, text2\n",
    "img1_weight = weight[:, 0 : proj_dim]\n",
    "txt1_weight = weight[:, proj_dim : proj_dim * 2]\n",
    "img2_weight = weight[:, proj_dim * 2 : proj_dim * 3]\n",
    "txt2_weight = weight[:, proj_dim * 3 : proj_dim * 4]\n",
    "\n",
    "# L2 Norm으로 평균 중요도 측정\n",
    "def mean_l2_norm(w): return np.linalg.norm(w, axis=1).mean()\n",
    "\n",
    "results = {\n",
    "    \"Image1 Weight Norm\": mean_l2_norm(img1_weight),\n",
    "    \"Text1  Weight Norm\": mean_l2_norm(txt1_weight),\n",
    "    \"Image2 Weight Norm\": mean_l2_norm(img2_weight),\n",
    "    \"Text2  Weight Norm\": mean_l2_norm(txt2_weight),\n",
    "}\n",
    "\n",
    "# 출력\n",
    "print(\"=== Modality별 Weight 영향도 (1st Linear Layer 기준) ===\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9e17e-2ed2-4b3f-a6c3-07b994818573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4858730-f736-4b6c-b826-63ae42cbfa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6eaa63-6356-4467-9d75-d0042cd54610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Modality별 Weight 영향도 (1st Linear Layer 기준) ===\n",
      "Image1 Weight Norm: 0.2896\n",
      "Text1  Weight Norm: 0.3023\n",
      "Image2 Weight Norm: 0.2893\n",
      "Text2  Weight Norm: 0.3016\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPModel\n",
    "\n",
    "# 1. CLIP 모델 불러오기\n",
    "clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# 2. 전체 모델 구조 정의\n",
    "class FullModel(torch.nn.Module):\n",
    "    def __init__(self, clip_model, proj_dim=512):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(proj_dim * 4, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image1_feat, text1_feat, image2_feat, text2_feat):\n",
    "        x = torch.cat([image1_feat, text1_feat, image2_feat, text2_feat], dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# 3. 모델 인스턴스 생성 및 가중치 로드\n",
    "proj_dim = 512\n",
    "model = FullModel(clip_model=clip, proj_dim=proj_dim)\n",
    "\n",
    "# ⚠️ 저장된 모델 가중치 불러오기 (FullModel 기준)\n",
    "model_path = r\"D:\\Project\\PJT_10\\saved_models\\clip_pair_best_epoch2_20250717_001936.pth\"\n",
    "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)  # 이제 정상 로드될 것\n",
    "\n",
    "# 4. classifier의 첫 Linear Layer 가중치 분석\n",
    "first_linear = model.classifier[0]  # Linear(2048 → 512)\n",
    "weight = first_linear.weight.data.cpu().numpy()\n",
    "\n",
    "# 5. 모달리티별 weight 분리\n",
    "img1_weight = weight[:, 0 : proj_dim]\n",
    "txt1_weight = weight[:, proj_dim : proj_dim * 2]\n",
    "img2_weight = weight[:, proj_dim * 2 : proj_dim * 3]\n",
    "txt2_weight = weight[:, proj_dim * 3 : proj_dim * 4]\n",
    "\n",
    "# 6. L2 Norm 계산\n",
    "def mean_l2_norm(w): return np.linalg.norm(w, axis=1).mean()\n",
    "\n",
    "results = {\n",
    "    \"Image1 Weight Norm\": mean_l2_norm(img1_weight),\n",
    "    \"Text1  Weight Norm\": mean_l2_norm(txt1_weight),\n",
    "    \"Image2 Weight Norm\": mean_l2_norm(img2_weight),\n",
    "    \"Text2  Weight Norm\": mean_l2_norm(txt2_weight),\n",
    "}\n",
    "\n",
    "# 7. 출력\n",
    "print(\"=== Modality별 Weight 영향도 (1st Linear Layer 기준) ===\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98738a95-678f-46f0-856b-292cfdf0defc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf5c9c-b3c3-48e2-8175-e84f3853b762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
