{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dc2562-efb3-4094-8138-7e1de9287e02",
   "metadata": {},
   "source": [
    "# ⭐ Shopee - Price Match Guarantee\n",
    "- Determine if two products are the same by their images\n",
    "- https://www.kaggle.com/competitions/shopee-product-matching/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f9dae-c8c5-4160-b0e2-b50577d89d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdc2b064-30d9-4e71-b6de-995be96b4459",
   "metadata": {},
   "source": [
    "### Shopee - Price Match Guarantee: 문제 개요 및 태스크 요약\n",
    "\n",
    "---\n",
    "\n",
    "#### 태스크 목적\n",
    "\n",
    "- 두 제품이 동일한 상품인지 **이미지와 텍스트** 정보를 활용해 자동으로 판단하는 멀티모달 제품 매칭 문제입니다.\n",
    "- 동일 제품임에도 사진, 제목 등이 다를 수 있어 단순 비교가 어렵고, 이를 딥러닝 기반으로 해결하는 것이 목표입니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 데이터 및 문제 특성\n",
    "\n",
    "- 각 제품은 `image` (사진)와 `title` (상품명) 텍스트를 가집니다.\n",
    "- 제품들은 `label_group`으로 묶여 있으며, 같은 그룹은 같은 제품을 의미합니다.\n",
    "- 제품 쌍(positive/negative)을 만들어 **두 제품이 같은 그룹인지 여부**를 이진 분류합니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 구현한 코드의 주요 변경 및 특징\n",
    "\n",
    "- **멀티모달 입력**: 이미지와 텍스트를 모두 입력으로 받는 Siamese 네트워크 구조를 사용  \n",
    "  → 각 쌍에서 두 제품의 이미지 임베딩과 텍스트 임베딩을 CLIP 모델로 추출하여 결합\n",
    "\n",
    "- **그룹 단위로 train/val/test 분할**  \n",
    "  → 같은 그룹에 속한 제품들은 같은 split에 배정하여 정보 누출 방지\n",
    "\n",
    "- **Positive / Negative 쌍 생성**  \n",
    "  → 같은 그룹에서 조합 가능한 제품 쌍은 positive (1)  \n",
    "  → 다른 그룹 간 제품 쌍은 negative (0), negative 쌍은 positive 쌍 대비 최대 2배 비율로 생성\n",
    "\n",
    "- **학습 목표**:  \n",
    "  - 입력: 두 제품의 이미지와 텍스트  \n",
    "  - 출력: 두 제품이 같은 상품인지 여부 (확률)  \n",
    "  - 손실 함수: `BCEWithLogitsLoss`를 사용하여 이진 분류 학습\n",
    "\n",
    "- **평가**:  \n",
    "  - 테스트셋에서 정확도와 손실을 계산  \n",
    "  - 실제 대회 평가 지표는 mean F1 score이나, 코드는 정확도를 기준으로 평가\n",
    "\n",
    "---\n",
    "\n",
    "#### 기대 효과 및 활용처\n",
    "\n",
    "- 제품 매칭 자동화로 잘못된 상품 중복이나 오인 문제 감소  \n",
    "- 쇼핑 플랫폼에서 정확한 상품 분류와 가격 비교 지원  \n",
    "- 소비자는 최적의 가격과 상품 정보를 더 쉽게 찾을 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "#### 참고\n",
    "\n",
    "- Shopee의 주요 동남아시아 및 대만 전자상거래 플랫폼  \n",
    "- 문제는 2021년 Kaggle 대회 `Shopee Product Matching` 기반\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d901b-c20f-42fa-a776-18cd44b52eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f09825-df5c-4270-84c5-b3f309495924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "036dacb0-f804-4dc8-a851-d5807e6edc19",
   "metadata": {},
   "source": [
    "# ⭐ 멀티모달 모델 (이미지 + 텍스트)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48024da3-7e18-4d9a-a97d-02c2c0fd0818",
   "metadata": {},
   "source": [
    "### 모델 개요: 이미지+텍스트를 모두 활용하는 멀티모달 Siamese 모델\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 입력 (Input)\n",
    "\n",
    "- 한 쌍(pair)의 데이터가 들어갑니다.\n",
    "- 각 데이터는 **이미지 + 텍스트**로 구성되어 있습니다.\n",
    "- 구체적으로 입력은 다음과 같습니다:\n",
    "  - `input_ids1`, `attention_mask1`, `pixel_values1`: 첫 번째 샘플의 텍스트 토큰, 어텐션 마스크, 이미지 픽셀값\n",
    "  - `input_ids2`, `attention_mask2`, `pixel_values2`: 두 번째 샘플의 텍스트 토큰, 어텐션 마스크, 이미지 픽셀값\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 모델 내부 처리\n",
    "\n",
    "- CLIP 모델을 사용하여 텍스트와 이미지를 각각 임베딩합니다.\n",
    "  - `get_text_features()` → 텍스트 임베딩 벡터 (예: 512차원)\n",
    "  - `get_image_features()` → 이미지 임베딩 벡터 (예: 512차원)\n",
    "- 각 샘플에서 이미지 임베딩과 텍스트 임베딩을 연결(concatenate)하여 하나의 벡터로 만듭니다.\n",
    "- 두 샘플의 벡터를 다시 연결(concatenate)하여 `(projection_dim * 4)` 차원의 벡터를 생성합니다.\n",
    "- 이 벡터를 분류기(classifier)에 통과시켜 두 샘플의 유사성 점수를 출력합니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 출력 (Output)\n",
    "\n",
    "- 모델의 출력은 두 샘플이 같은 그룹인지 아닌지에 대한 **로짓(logit)** 값입니다.\n",
    "- 이 값을 시그모이드 함수에 통과시켜 확률로 변환할 수 있습니다.\n",
    "- 확률이 0.5 이상이면 \"같은 그룹(positive pair)\"으로 예측합니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 요약\n",
    "\n",
    "| 구분      | 내용                                           |\n",
    "| --------- | ---------------------------------------------- |\n",
    "| 입력      | 두 샘플 (이미지1 + 텍스트1, 이미지2 + 텍스트2) |\n",
    "| 내부 처리 | CLIP 모델로 이미지와 텍스트 각각 임베딩 후 concat |\n",
    "| 출력      | 두 샘플의 유사성 점수 (로짓, 확률로 변환 가능)   |\n",
    "\n",
    "---\n",
    "\n",
    "이 모델은 이미지와 텍스트 두 정보를 모두 활용하여 제품 쌍의 유사성을 판단하는 **멀티모달 쌍별 분류기**입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff027f8-7d58-40a4-bcfb-4ef196ee3bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5df5fd-cef3-43cb-839e-d756b7f4c4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd50402f-2f25-4bd9-8033-9c7c336ae203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 20392, Val size: 6820, Test size: 7038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|████████████████████████████████████| 4656/4656 [1:12:38<00:00,  1.07it/s]\n",
      "Validation Epoch 1: 100%|█████████████████████████████████| 1456/1456 [15:29<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.2741 | Train Acc: 0.8852 | Val Loss: 0.3011 | Val Acc: 0.8963\n",
      "✅ Saved best model at epoch 1 to ./saved_models\\clip_pair_best_epoch1_20250716_225154.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|████████████████████████████████████| 4656/4656 [1:12:13<00:00,  1.07it/s]\n",
      "Validation Epoch 2: 100%|█████████████████████████████████| 1456/1456 [15:27<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.1232 | Train Acc: 0.9561 | Val Loss: 0.2890 | Val Acc: 0.9048\n",
      "✅ Saved best model at epoch 2 to ./saved_models\\clip_pair_best_epoch2_20250717_001936.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|████████████████████████████████████| 4656/4656 [1:12:11<00:00,  1.07it/s]\n",
      "Validation Epoch 3: 100%|█████████████████████████████████| 1456/1456 [15:29<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.0930 | Train Acc: 0.9681 | Val Loss: 0.3004 | Val Acc: 0.9103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|████████████████████████████████████| 4656/4656 [1:12:07<00:00,  1.08it/s]\n",
      "Validation Epoch 4: 100%|█████████████████████████████████| 1456/1456 [15:29<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.0711 | Train Acc: 0.9760 | Val Loss: 0.3620 | Val Acc: 0.9017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|████████████████████████████████████| 4656/4656 [1:12:01<00:00,  1.08it/s]\n",
      "Validation Epoch 5: 100%|█████████████████████████████████| 1456/1456 [15:24<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.0583 | Train Acc: 0.9805 | Val Loss: 0.4440 | Val Acc: 0.9031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|████████████████████████████████████| 4656/4656 [1:11:58<00:00,  1.08it/s]\n",
      "Validation Epoch 6: 100%|█████████████████████████████████| 1456/1456 [15:25<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.0483 | Train Acc: 0.9844 | Val Loss: 0.4180 | Val Acc: 0.9043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|████████████████████████████████████| 4656/4656 [1:12:01<00:00,  1.08it/s]\n",
      "Validation Epoch 7: 100%|█████████████████████████████████| 1456/1456 [15:24<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.0408 | Train Acc: 0.9869 | Val Loss: 0.5381 | Val Acc: 0.9014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|████████████████████████████████████| 4656/4656 [1:11:57<00:00,  1.08it/s]\n",
      "Validation Epoch 8: 100%|█████████████████████████████████| 1456/1456 [15:26<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.0364 | Train Acc: 0.9884 | Val Loss: 0.5069 | Val Acc: 0.9062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: 100%|████████████████████████████████████| 4656/4656 [1:13:31<00:00,  1.06it/s]\n",
      "Validation Epoch 9: 100%|█████████████████████████████████| 1456/1456 [15:45<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.0307 | Train Acc: 0.9904 | Val Loss: 0.5506 | Val Acc: 0.9012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10: 100%|███████████████████████████████████| 4656/4656 [1:12:29<00:00,  1.07it/s]\n",
      "Validation Epoch 10: 100%|████████████████████████████████| 1456/1456 [15:28<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.0282 | Train Acc: 0.9913 | Val Loss: 0.6696 | Val Acc: 0.9000\n",
      "\n",
      "📊 Training log saved to ./saved_models\\training_log_20250717_120149.csv\n",
      "✅ Loaded best model from ./saved_models\\clip_pair_best_epoch2_20250717_001936.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████| 1741/1741 [18:22<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Test Loss: 0.3174 | Test Accuracy: 0.9016\n",
      "📝 Test results saved to ./saved_models\\clip_pair_best_epoch2_20250717_001936_test_result.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 0. 라이브러리 임포트\n",
    "import os, random, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# 1. 설정\n",
    "DATA_DIR = r\"D:\\Project\\PJT_10\\shopee-product-matching\"\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "IMG_DIR = os.path.join(DATA_DIR, \"train_images\")\n",
    "\n",
    "SAVE_DIR = \"./saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "# 2. 데이터 로딩 및 라벨 인코딩\n",
    "df = pd.read_csv(CSV_PATH).reset_index(drop=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label_encoded\"] = label_encoder.fit_transform(df[\"label_group\"])\n",
    "\n",
    "# 3. 그룹 단위로 train/val/test split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.4, random_state=SEED)\n",
    "train_idx, temp_idx = next(gss.split(df, groups=df[\"label_encoded\"]))\n",
    "\n",
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "temp_df = df.iloc[temp_idx].reset_index(drop=True)\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=SEED)\n",
    "val_idx, test_idx = next(gss2.split(temp_df, groups=temp_df[\"label_encoded\"]))\n",
    "\n",
    "val_df = temp_df.iloc[val_idx].reset_index(drop=True)\n",
    "test_df = temp_df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# 4. Positive / Negative pair 생성 함수\n",
    "def create_pairs(df, max_neg_per_pos=2):\n",
    "    pairs = []\n",
    "    label_groups = df[\"label_encoded\"].unique()\n",
    "\n",
    "    for lg in label_groups:\n",
    "        group_df = df[df[\"label_encoded\"] == lg]\n",
    "        if len(group_df) < 2:\n",
    "            continue\n",
    "        idxs = group_df.index.tolist()\n",
    "        pos_combs = list(combinations(idxs, 2))\n",
    "        for i, j in pos_combs:\n",
    "            pairs.append((i, j, 1))\n",
    "\n",
    "    pos_count = sum(1 for _,_,label in pairs if label == 1)\n",
    "    neg_needed = pos_count * max_neg_per_pos\n",
    "\n",
    "    all_indices = list(df.index)  # <- 여기 변경\n",
    "    neg_pairs = set()\n",
    "\n",
    "    while len(neg_pairs) < neg_needed:\n",
    "        i, j = random.sample(all_indices, 2)\n",
    "        if df.loc[i, \"label_encoded\"] != df.loc[j, \"label_encoded\"]:\n",
    "            neg_pairs.add((i, j))\n",
    "    for i, j in neg_pairs:\n",
    "        pairs.append((i, j, 0))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "# 5. Dataset 클래스\n",
    "class ShopeePairDataset(Dataset):\n",
    "    def __init__(self, df, pairs, img_dir, processor):\n",
    "        self.df = df\n",
    "        self.pairs = pairs\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i1, i2, label = self.pairs[idx]\n",
    "        row1 = self.df.loc[i1]\n",
    "        row2 = self.df.loc[i2]\n",
    "\n",
    "        image1 = Image.open(os.path.join(self.img_dir, row1[\"image\"])).convert(\"RGB\")\n",
    "        image2 = Image.open(os.path.join(self.img_dir, row2[\"image\"])).convert(\"RGB\")\n",
    "\n",
    "        text1 = row1[\"title\"]\n",
    "        text2 = row2[\"title\"]\n",
    "\n",
    "        return {\"image1\": image1, \"text1\": text1,\n",
    "                \"image2\": image2, \"text2\": text2,\n",
    "                \"label\": label}\n",
    "\n",
    "# 6. Collate 함수\n",
    "def collate_fn(batch):\n",
    "    texts1 = [item[\"text1\"] for item in batch]\n",
    "    texts2 = [item[\"text2\"] for item in batch]\n",
    "    images1 = [item[\"image1\"] for item in batch]\n",
    "    images2 = [item[\"image2\"] for item in batch]\n",
    "    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.float)\n",
    "\n",
    "    inputs1 = processor(text=texts1, images=images1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs2 = processor(text=texts2, images=images2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    return {\n",
    "        \"input_ids1\": inputs1[\"input_ids\"],\n",
    "        \"attention_mask1\": inputs1[\"attention_mask\"],\n",
    "        \"pixel_values1\": inputs1[\"pixel_values\"],\n",
    "\n",
    "        \"input_ids2\": inputs2[\"input_ids\"],\n",
    "        \"attention_mask2\": inputs2[\"attention_mask\"],\n",
    "        \"pixel_values2\": inputs2[\"pixel_values\"],\n",
    "\n",
    "        \"label\": labels\n",
    "    }\n",
    "\n",
    "# 7. 모델 정의 (Siamese)\n",
    "class CLIPSiameseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(model_name)\n",
    "        # 모델 마지막 레이어\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.clip.config.projection_dim * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, pixel_values1,\n",
    "                      input_ids2, attention_mask2, pixel_values2):\n",
    "        text_features1 = self.clip.get_text_features(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "        image_features1 = self.clip.get_image_features(pixel_values=pixel_values1)\n",
    "        feat1 = torch.cat([image_features1, text_features1], dim=1)\n",
    "\n",
    "        text_features2 = self.clip.get_text_features(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "        image_features2 = self.clip.get_image_features(pixel_values=pixel_values2)\n",
    "        feat2 = torch.cat([image_features2, text_features2], dim=1)\n",
    "\n",
    "        combined = torch.cat([feat1, feat2], dim=1)\n",
    "        output = self.classifier(combined).squeeze(1)\n",
    "        return output\n",
    "\n",
    "# 8. 데이터셋 및 데이터로더 준비\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "train_pairs = create_pairs(train_df)\n",
    "val_pairs = create_pairs(val_df)\n",
    "test_pairs = create_pairs(test_df)\n",
    "\n",
    "train_dataset = ShopeePairDataset(train_df, train_pairs, IMG_DIR, processor)\n",
    "val_dataset = ShopeePairDataset(val_df, val_pairs, IMG_DIR, processor)\n",
    "test_dataset = ShopeePairDataset(test_df, test_pairs, IMG_DIR, processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 9. 학습 루프 및 평가\n",
    "model = CLIPSiameseModel(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "log_list = []\n",
    "\n",
    "patience = 3  # 개선 없을 때 최대 허용 epoch 수\n",
    "counter = 0   # 얼리 스탑 카운터\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"label\"}\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        probs = torch.sigmoid(outputs)        # logits → 확률\n",
    "        preds = (probs >= 0.5).float()        # threshold 적용\n",
    "        \n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "        \n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"label\"}\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            \n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    log_list.append({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc\n",
    "    })\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        save_path = os.path.join(SAVE_DIR, f\"clip_pair_best_epoch{epoch+1}_{timestamp}.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"✅ Saved best model at epoch {epoch+1} to {save_path}\")\n",
    "\n",
    "log_df = pd.DataFrame(log_list)\n",
    "log_csv_path = os.path.join(SAVE_DIR, f\"training_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "log_df.to_csv(log_csv_path, index=False)\n",
    "print(f\"\\n📊 Training log saved to {log_csv_path}\")\n",
    "    \n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"label\"}\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            \n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    \n",
    "    print(f\"\\n🧪 Test Loss: {avg_loss:.4f} | Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # 모델 파일명 기반 저장 이름 생성\n",
    "    model_name = os.path.splitext(os.path.basename(best_model_path))[0]  # 'clip_pair_best_epoch3_20250715_235911'\n",
    "    result_path = os.path.join(SAVE_DIR, f\"{model_name}_test_result.txt\")\n",
    "\n",
    "    # 결과 파일 저장\n",
    "    with open(result_path, \"w\") as f:\n",
    "        f.write(f\"Test Loss: {avg_loss:.4f}\\n\")\n",
    "        f.write(f\"Test Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "    print(f\"📝 Test results saved to {result_path}\")\n",
    "\n",
    "\n",
    "best_models = sorted([f for f in os.listdir(SAVE_DIR) if f.startswith(\"clip_pair_best\") and f.endswith(\".pth\")])\n",
    "best_model_path = os.path.join(SAVE_DIR, best_models[-1])\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "print(f\"✅ Loaded best model from {best_model_path}\")\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93d30b-c67b-4887-a96d-4332da41d0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b89e2b3-4b87-4452-b9eb-7cb4898966a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████| 1741/1741 [18:21<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Test Loss: 0.3174 | Test Accuracy: 0.9016\n",
      "📝 Test results saved to ./saved_models\\clip_pair_best_epoch2_20250717_001936_test_result.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing F1: 100%|███████████████████████████████████████| 1741/1741 [18:23<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test F1-score: 0.8447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"label\"}\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            \n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    \n",
    "    print(f\"\\n🧪 Test Loss: {avg_loss:.4f} | Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # 모델 파일명 기반 저장 이름 생성\n",
    "    model_name = os.path.splitext(os.path.basename(best_model_path))[0]  # 'clip_pair_best_epoch3_20250715_235911'\n",
    "    result_path = os.path.join(SAVE_DIR, f\"{model_name}_test_result.txt\")\n",
    "\n",
    "    # 결과 파일 저장\n",
    "    with open(result_path, \"w\") as f:\n",
    "        f.write(f\"Test Loss: {avg_loss:.4f}\\n\")\n",
    "        f.write(f\"Test Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "    print(f\"📝 Test results saved to {result_path}\")\n",
    "\n",
    "best_models = sorted([f for f in os.listdir(SAVE_DIR) if f.startswith(\"clip_pair_best\") and f.endswith(\".pth\")])\n",
    "best_model_path = os.path.join(SAVE_DIR, best_models[-1])\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "evaluate(model, test_loader)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Computing F1\"):\n",
    "        inputs = {k: v.to(DEVICE) for k, v in batch.items() if k != \"label\"}\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs >= 0.5).float()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# f1-score 계산 (이진 분류라면 'binary')\n",
    "f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "print(f\"✅ Test F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb0f52-9841-40a7-a336-5da82d87f35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c420a4-f01d-4bb0-afe5-1f6932ec87d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62423ebd-24c1-4677-b86b-e47f056a52fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e01fe6-88e4-436e-81e4-24209cd3d280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216eff55-ff6c-4b4b-9172-0fdc84495eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
