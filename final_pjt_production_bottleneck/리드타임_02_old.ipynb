{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71408559-ba42-4840-873f-0fad3f4da8ee",
   "metadata": {},
   "source": [
    "# 1) 설정 · 로드 · 공통 유틸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a24eaa-3a84-4616-ab8c-fb90be997a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ===============================================================\n",
    "# 개선 패키지: SKU 개별 & 통합 리드타임, 병목, 생산량 + 탄력도(민감도, 경량)\n",
    "# - 입력: CSV 경로 (기본: \"./Final Results Extended.csv\")\n",
    "# - 산출: ./outputs/ 아래에 모든 결과 저장\n",
    "# - 주요 개선:\n",
    "#   1) 생산량 타깃 누수 제거(c_TotalProducts 피처 제외)\n",
    "#   2) 행별 병목 라벨 생성 → SKU별 리드타임과 조건부 연계표 저장\n",
    "#   3) 리드타임/생산량 탄력도: (a) 빠른 유한차분법(FD) + 샘플링, (b) (옵션) PDP\n",
    "#   4) What-if 시뮬레이터 (통합 & SKU별)\n",
    "#   5) (옵션) 결함(불량) 타깃 자동 탐지 동일 파이프라인\n",
    "#   6) (옵션) mu/sd 기반 편차 파생피처\n",
    "# ===============================================================\n",
    "\n",
    "import os, re, json, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# ---- 경로/표시 설정 ----\n",
    "CSV_PATH = \"./Final Results Extended.csv\"   # 필요시 변경\n",
    "OUT_DIR  = \"./outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 160)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "# ---- 데이터 로드 ----\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "print(\"=== Dataset shape ===\", df.shape)\n",
    "display(df.head(3))\n",
    "\n",
    "# ---- 공통 유틸 ----\n",
    "def to_numeric_df(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    return d.apply(pd.to_numeric, errors=\"coerce\").fillna(0.0).astype(\"float32\")\n",
    "\n",
    "def savefig(path: str):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, bbox_inches=\"tight\", dpi=140)\n",
    "    plt.show()\n",
    "\n",
    "def metrics_dict(y_true, y_pred) -> dict:\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    r2   = float(r2_score(y_true, y_pred))\n",
    "    return {\"MAE\": round(mae,3), \"RMSE\": round(rmse,3), \"R2\": round(r2,4)}\n",
    "\n",
    "def train_rf(X: pd.DataFrame, y: np.ndarray, random_state=42, n_estimators=300, test_size=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_estimators, max_depth=None, random_state=random_state, n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    m = metrics_dict(y_test, y_pred)\n",
    "    fi = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    pi = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=random_state, n_jobs=-1)\n",
    "    pi_s = pd.Series(pi.importances_mean, index=X.columns).sort_values(ascending=False)\n",
    "    return rf, (X_train, X_test, y_train, y_test), m, fi, pi_s\n",
    "\n",
    "def train_ridge(X: pd.DataFrame, y: np.ndarray, alpha=1.0, random_state=42, test_size=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    reg = Ridge(alpha=alpha, random_state=random_state)\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    m = metrics_dict(y_test, y_pred)\n",
    "    coef = pd.Series(reg.coef_, index=X.columns).sort_values(ascending=False)\n",
    "    std_x = X_test.std(axis=0).replace(0, np.nan)\n",
    "    std_y = np.std(y_test) if np.std(y_test) > 0 else np.nan\n",
    "    beta_std = (coef * (std_x / std_y)).sort_values(ascending=False)\n",
    "    return reg, (X_train, X_test, y_train, y_test), m, coef, beta_std\n",
    "\n",
    "def topk(s: pd.Series, k=20):\n",
    "    return s.head(k) if len(s) > k else s\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b92fa-6b11-4b7b-9ad4-882748d157e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "820bae56-ed53-49c7-b029-f8969d4c76ec",
   "metadata": {},
   "source": [
    "# 2) 컬럼 인식 · SKU 타깃 · 통합 리드타임(메모리 안전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6aa294-bb58-47e9-84a9-eeef86446365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 컬럼 그룹핑 ===\n",
    "queue_cols = [c for c in df.columns if c.endswith(\"_Queue\")]\n",
    "util_cols  = [c for c in df.columns if c.endswith(\"_Util\")]\n",
    "cycle_cols = [c for c in df.columns if c.startswith(\"c_Cycle\")]\n",
    "cell_cols  = [c for c in df.columns if c.startswith(\"c_Cell\")]\n",
    "\n",
    "print(\"\\n=== Column groups ===\")\n",
    "print(f\"Queue cols  ({len(queue_cols)}): {queue_cols[:10]}{' ...' if len(queue_cols)>10 else ''}\")\n",
    "print(f\"Util cols   ({len(util_cols)}):  {util_cols[:10]}{' ...' if len(util_cols)>10 else ''}\")\n",
    "print(f\"Cycle cols  ({len(cycle_cols)}): {cycle_cols[:10]}\")\n",
    "print(f\"Cell cols   ({len(cell_cols)}):  {cell_cols[:10]}{' ...' if len(cell_cols)>10 else ''}\")\n",
    "\n",
    "# === SKU 자동 탐지 ===\n",
    "sku_ids = sorted({re.findall(r\"SKU(\\d+)_\", c)[0] for c in df.columns if re.findall(r\"SKU(\\d+)_\", c)})\n",
    "print(\"\\n=== Detected SKUs ===\", sku_ids)\n",
    "\n",
    "# 리드타임 컬럼 후보\n",
    "LT_PATTERNS = dict(\n",
    "    VA=[\"_VA_Time_sec\", \"_VA_Time\", \"_VA\"],\n",
    "    WAIT=[\"_Wait_Time_sec\", \"_Wait_Time\", \"_Wait\"],\n",
    "    LIFT=[\"_Transport_Time_sec\", \"_Transport_Time\", \"_Transport\", \"_Lift\"],\n",
    "    TOTAL=[\"_Total_sec\", \"_LeadTime_sec\", \"_LeadTime\"]\n",
    ")\n",
    "def find_first_existing(base: str, suffixes: List[str]) -> Optional[str]:\n",
    "    for s in suffixes:\n",
    "        col = base + s\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# === SKU별 리드타임/생산량 타깃 ===\n",
    "sku_targets = {}\n",
    "for sid in sku_ids:\n",
    "    base = f\"SKU{sid}\"\n",
    "    col_VA   = find_first_existing(base, LT_PATTERNS[\"VA\"])\n",
    "    col_WAIT = find_first_existing(base, LT_PATTERNS[\"WAIT\"])\n",
    "    col_LIFT = find_first_existing(base, LT_PATTERNS[\"LIFT\"])\n",
    "    col_TOT  = find_first_existing(base, LT_PATTERNS[\"TOTAL\"])\n",
    "\n",
    "    # 리드타임 타깃\n",
    "    if col_TOT:\n",
    "        lt_series = pd.to_numeric(df[col_TOT], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n",
    "        lt_name, lt_method = col_TOT, \"TOTAL_col\"\n",
    "    else:\n",
    "        VA   = pd.to_numeric(df[col_VA],   errors=\"coerce\").fillna(0.0).astype(\"float32\") if col_VA   else 0.0\n",
    "        WAIT = pd.to_numeric(df[col_WAIT], errors=\"coerce\").fillna(0.0).astype(\"float32\") if col_WAIT else 0.0\n",
    "        LIFT = pd.to_numeric(df[col_LIFT], errors=\"coerce\").fillna(0.0).astype(\"float32\") if col_LIFT else 0.0\n",
    "        lt_series = pd.Series(VA, index=df.index) + pd.Series(WAIT, index=df.index) + pd.Series(LIFT, index=df.index)\n",
    "        lt_series = lt_series.astype(\"float32\")\n",
    "        lt_name, lt_method = f\"{base}_LeadTime_composed\", \"VA+WAIT+LIFT\"\n",
    "\n",
    "    # 생산량 타깃: 해당 SKU의 셀 출력 합\n",
    "    cell_cols_sku = [c for c in cell_cols if re.search(fr\"__SKU{sid}\\b\", c)]\n",
    "    if cell_cols_sku:\n",
    "        prod_series = to_numeric_df(df[cell_cols_sku]).sum(axis=1).astype(\"float32\")\n",
    "        prod_name   = f\"Total_SKU{sid}\"\n",
    "    else:\n",
    "        prod_series = pd.Series(0.0, index=df.index, dtype=\"float32\")\n",
    "        prod_name   = f\"Total_SKU{sid}_zeros\"\n",
    "\n",
    "    sku_targets[sid] = dict(\n",
    "        lead_time=lt_series, lead_time_name=lt_name, lead_time_method=lt_method,\n",
    "        production=prod_series, production_name=prod_name, cell_cols=cell_cols_sku\n",
    "    )\n",
    "\n",
    "print(\"\\n=== SKU targets overview (first 2) ===\")\n",
    "for sid in sku_ids[:2]:\n",
    "    print(f\"SKU{sid} LT: {sku_targets[sid]['lead_time_name']} ({sku_targets[sid]['lead_time_method']}), \"\n",
    "          f\"PROD: {sku_targets[sid]['production_name']} | #cell_cols={len(sku_targets[sid]['cell_cols'])}\")\n",
    "\n",
    "# === 통합 타깃 ===\n",
    "# 생산량(통합)\n",
    "if \"c_TotalProducts\" in df.columns:\n",
    "    total_prod = pd.to_numeric(df[\"c_TotalProducts\"], errors=\"coerce\").fillna(0.0).astype(\"float32\")\n",
    "else:\n",
    "    total_prod = to_numeric_df(df[cell_cols]).sum(axis=1).astype(\"float32\")\n",
    "\n",
    "# 리드타임(생산량 가중평균, 메모리 안전)\n",
    "N = len(df)\n",
    "numerator = np.zeros(N, dtype=np.float32)  # Σ(lt_k * prod_k)\n",
    "weights   = np.zeros(N, dtype=np.float32)  # Σ(prod_k)\n",
    "sum_lt    = np.zeros(N, dtype=np.float32)  # fallback 평균 분자\n",
    "k_count   = 0\n",
    "\n",
    "for sid in sku_ids:\n",
    "    lt = sku_targets[sid][\"lead_time\"].values\n",
    "    pr = sku_targets[sid][\"production\"].values\n",
    "    numerator += lt * pr\n",
    "    weights   += pr\n",
    "    sum_lt    += lt\n",
    "    k_count   += 1\n",
    "\n",
    "fallback_mean = (sum_lt / max(k_count, 1)).astype(\"float32\")\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    weighted_lt = np.where(weights > 0.0, numerator / weights, fallback_mean)\n",
    "agg_lead_time = pd.Series(weighted_lt, index=df.index, name=\"Agg_LeadTime_weighted\").astype(\"float32\")\n",
    "\n",
    "del numerator, weights, sum_lt; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23a54f-4ddb-4383-a60d-16b322b73c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb1d368c-f690-4dc3-acbc-dc3e69d84784",
   "metadata": {},
   "source": [
    "# 3) 병목 라벨링 · 요약표(전체/통합) · SKU×병목 연계표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab884ea-87aa-4f9d-81d9-aa24d568ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 행별 병목 라벨링 ===\n",
    "Q = to_numeric_df(df[[c for c in df.columns if c.endswith(\"_Queue\")]])\n",
    "if Q.shape[1] == 0:\n",
    "    raise ValueError(\"Queue 계열 컬럼이 없습니다.\")\n",
    "df[\"_bneck\"] = Q.idxmax(axis=1)  # 각 행 최대 Queue의 컬럼명\n",
    "\n",
    "def compute_bneck_stats(target: pd.Series, name: str, out_csv: str):\n",
    "    tmp = pd.DataFrame({\"bneck\": df[\"_bneck\"].astype(\"category\"), \"val\": target.astype(\"float32\")})\n",
    "    stats = tmp.groupby(\"bneck\").agg(\n",
    "        top1_count=(\"bneck\", \"count\"),\n",
    "        mean_target=(\"val\", \"mean\"),\n",
    "        std_target=(\"val\", \"std\")\n",
    "    ).sort_values(\"top1_count\", ascending=False).reset_index()\n",
    "    stats[\"target_name\"] = name\n",
    "    stats.to_csv(ensure_dir(os.path.join(OUT_DIR, out_csv)), index=False)\n",
    "    return stats\n",
    "\n",
    "bneck_total_prod = compute_bneck_stats(total_prod, \"TotalProduction\", \"bottleneck_totalProduction.csv\")\n",
    "bneck_agg_lt     = compute_bneck_stats(agg_lead_time, \"AggLeadTime\", \"bottleneck_aggLeadTime.csv\")\n",
    "\n",
    "display(bneck_total_prod.head(10))\n",
    "display(bneck_agg_lt.head(10))\n",
    "\n",
    "# === SKU별: 병목 ↔ 리드타임 조건부 연계표 ===\n",
    "TOP_BNECKS = list(bneck_total_prod[\"bneck\"].head(8).values)  # 빈도 상위 N\n",
    "for sid in sku_ids:\n",
    "    lt = sku_targets[sid][\"lead_time\"].astype(\"float32\")\n",
    "    tmp = pd.DataFrame({\"bneck\": df[\"_bneck\"], \"lt\": lt})\n",
    "    tmp[\"bneck_top\"] = np.where(tmp[\"bneck\"].isin(TOP_BNECKS), tmp[\"bneck\"], \"Other\")\n",
    "    grp = tmp.groupby(\"bneck_top\")[\"lt\"].agg([\"count\",\"mean\",\"std\"]).sort_values(\"count\", ascending=False)\n",
    "    grp.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"sku_bneck_effect_SKU{sid}.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ec50f-5863-4b60-9a05-2a37d4ae2c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3929b07-6309-4514-9a9c-d54423a45755",
   "metadata": {},
   "source": [
    "# 4) 피처 구성(누수 방지 + 병목 One-Hot) · 모델 학습(RF & Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965bee9-5396-449a-b88a-77ae16addee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 피처 구성 ===\n",
    "time_like_regex = re.compile(r\"(?:_Time|_sec|_LeadTime|_Total)\", re.IGNORECASE)\n",
    "\n",
    "def build_features_for_leadtime(df_: pd.DataFrame, exclude_cols: List[str],\n",
    "                                add_bneck_onehot=True, top_bneck_list=None) -> pd.DataFrame:\n",
    "    cols = []\n",
    "    for c in df_.columns:\n",
    "        if c in exclude_cols or c == \"Time_Now\":\n",
    "            continue\n",
    "        if time_like_regex.search(c):      # 시간/리드타임 직접 계열 제외(누수 방지)\n",
    "            continue\n",
    "        cols.append(c)\n",
    "    X = to_numeric_df(df_[cols]) if cols else pd.DataFrame(index=df_.index)\n",
    "    if add_bneck_onehot and \"_bneck\" in df_.columns:\n",
    "        b = df_[\"_bneck\"].astype(str)\n",
    "        if top_bneck_list is not None:\n",
    "            b = np.where(b.isin(top_bneck_list), b, \"Other\")\n",
    "            b = pd.Series(b, index=df_.index)\n",
    "        D = pd.get_dummies(b, prefix=\"BNECK\", dtype=\"float32\")\n",
    "        X = pd.concat([X, D], axis=1)\n",
    "    return X\n",
    "\n",
    "def build_features_for_production(df_: pd.DataFrame, exclude_cols: List[str]) -> pd.DataFrame:\n",
    "    cols = []\n",
    "    for c in df_.columns:\n",
    "        if c in exclude_cols or c == \"Time_Now\":\n",
    "            continue\n",
    "        if c.startswith(\"c_Cell\"):         # 직접 합산되는 생산량 열 제외\n",
    "            continue\n",
    "        if c == \"c_TotalProducts\":         # 통합 생산량 타깃 누수 제거\n",
    "            continue\n",
    "        cols.append(c)\n",
    "    return to_numeric_df(df_[cols]) if cols else pd.DataFrame(index=df_.index)\n",
    "\n",
    "# === 통합(AGG) 모델 ===\n",
    "# LeadTime(AGG)\n",
    "X_lt_agg = build_features_for_leadtime(df, exclude_cols=[], add_bneck_onehot=True, top_bneck_list=TOP_BNECKS)\n",
    "y_lt_agg = agg_lead_time.values\n",
    "rf_lt_agg, splits_lt_agg, m_rf_lt_agg, fi_lt_agg, pi_lt_agg = train_rf(X_lt_agg, y_lt_agg)\n",
    "rg_lt_agg, splits_rg_lt_agg, m_rg_lt_agg, coef_lt_agg, beta_std_lt_agg = train_ridge(X_lt_agg, y_lt_agg)\n",
    "\n",
    "fi_lt_agg.to_csv(ensure_dir(os.path.join(OUT_DIR, \"fi_leadtime_AGG.csv\")))\n",
    "pi_lt_agg.to_csv(ensure_dir(os.path.join(OUT_DIR, \"pi_leadtime_AGG.csv\")))\n",
    "coef_lt_agg.to_csv(ensure_dir(os.path.join(OUT_DIR, \"ridge_coef_leadtime_AGG.csv\")))\n",
    "beta_std_lt_agg.to_csv(ensure_dir(os.path.join(OUT_DIR, \"ridge_betaStd_leadtime_AGG.csv\")))\n",
    "plt.figure(figsize=(8,6)); topk(fi_lt_agg,20).iloc[::-1].plot(kind=\"barh\"); plt.title(\"RF FI (LeadTime) - AGG\"); savefig(os.path.join(OUT_DIR,\"fi_leadtime_AGG_top20.png\"))\n",
    "\n",
    "# Production(AGG)\n",
    "X_pr_agg = build_features_for_production(df, exclude_cols=[])\n",
    "y_pr_agg = total_prod.values\n",
    "rf_pr_agg, splits_pr_agg, m_rf_pr_agg, fi_pr_agg, pi_pr_agg = train_rf(X_pr_agg, y_pr_agg)\n",
    "rg_pr_agg, splits_rg_pr_agg, m_rg_pr_agg, coef_pr_agg, beta_std_pr_agg = train_ridge(X_pr_agg, y_pr_agg)\n",
    "\n",
    "fi_pr_agg.to_csv(ensure_dir(os.path.join(OUT_DIR, \"fi_production_AGG.csv\")))\n",
    "pi_pr_agg.to_csv(ensure_dir(os.path.join(OUT_DIR, \"pi_production_AGG.csv\")))\n",
    "coef_pr_agg.to_csv(ensure_dir(os.path.join(OUT_DIR, \"ridge_coef_production_AGG.csv\")))\n",
    "beta_std_pr_agg.to_csv(ensure_dir(os.path.join(OUT_DIR, \"ridge_betaStd_production_AGG.csv\")))\n",
    "plt.figure(figsize=(8,6)); topk(fi_pr_agg,20).iloc[::-1].plot(kind=\"barh\"); plt.title(\"RF FI (Production) - AGG\"); savefig(os.path.join(OUT_DIR,\"fi_production_AGG_top20.png\"))\n",
    "\n",
    "print(\"\\n=== AGG Metrics ===\")\n",
    "print({\"RF_LT\": m_rf_lt_agg, \"RG_LT\": m_rg_lt_agg, \"RF_PR\": m_rf_pr_agg, \"RG_PR\": m_rg_pr_agg})\n",
    "\n",
    "# === SKU별 모델 ===\n",
    "per_sku_results = {}\n",
    "for sid in sku_ids:\n",
    "    # LeadTime\n",
    "    y_lt = sku_targets[sid][\"lead_time\"].values\n",
    "    exclude_lt = set(sku_targets[sid][\"cell_cols\"])\n",
    "    exclude_lt |= {c for c in df.columns if c.startswith(f\"SKU{sid}_\") and time_like_regex.search(c)}\n",
    "    X_lt = build_features_for_leadtime(df, list(exclude_lt), add_bneck_onehot=True, top_bneck_list=TOP_BNECKS)\n",
    "\n",
    "    if X_lt.shape[1] > 0:\n",
    "        rf_lt, spl_lt, m_rf_lt, fi_lt, pi_lt = train_rf(X_lt, y_lt)\n",
    "        rg_lt, spl_rg_lt, m_rg_lt, coef_lt, beta_std_lt = train_ridge(X_lt, y_lt)\n",
    "        fi_lt.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"fi_leadtime_SKU{sid}.csv\")))\n",
    "        pi_lt.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"pi_leadtime_SKU{sid}.csv\")))\n",
    "        coef_lt.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"ridge_coef_leadtime_SKU{sid}.csv\")))\n",
    "        beta_std_lt.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"ridge_betaStd_leadtime_SKU{sid}.csv\")))\n",
    "        plt.figure(figsize=(8,6)); topk(fi_lt,20).iloc[::-1].plot(kind=\"barh\"); plt.title(f\"RF FI (LeadTime) - SKU{sid}\"); savefig(os.path.join(OUT_DIR, f\"fi_leadtime_SKU{sid}_top20.png\"))\n",
    "        plt.figure(figsize=(8,6)); topk(pi_lt,20).iloc[::-1].plot(kind=\"barh\"); plt.title(f\"Permutation FI (LeadTime) - SKU{sid}\"); savefig(os.path.join(OUT_DIR, f\"pi_leadtime_SKU{sid}_top20.png\"))\n",
    "        lt_pack = dict(rf=(rf_lt, spl_lt, m_rf_lt), rg=(rg_lt, spl_rg_lt, m_rg_lt),\n",
    "                       fi=fi_lt, pi=pi_lt, coef=coef_lt, beta_std=beta_std_lt, feats=list(X_lt.columns))\n",
    "    else:\n",
    "        print(f\"[WARN] SKU{sid}: LeadTime features empty → skip\")\n",
    "        lt_pack = None\n",
    "\n",
    "    # Production\n",
    "    y_pr = sku_targets[sid][\"production\"].values\n",
    "    exclude_pr = set(sku_targets[sid][\"cell_cols\"])\n",
    "    X_pr = build_features_for_production(df, list(exclude_pr))\n",
    "\n",
    "    if X_pr.shape[1] > 0:\n",
    "        rf_pr, spl_pr, m_rf_pr, fi_pr, pi_pr = train_rf(X_pr, y_pr)\n",
    "        rg_pr, spl_rg_pr, m_rg_pr, coef_pr, beta_std_pr = train_ridge(X_pr, y_pr)\n",
    "        fi_pr.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"fi_production_SKU{sid}.csv\")))\n",
    "        pi_pr.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"pi_production_SKU{sid}.csv\")))\n",
    "        coef_pr.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"ridge_coef_production_SKU{sid}.csv\")))\n",
    "        beta_std_pr.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"ridge_betaStd_production_SKU{sid}.csv\")))\n",
    "        plt.figure(figsize=(8,6)); topk(fi_pr,20).iloc[::-1].plot(kind=\"barh\"); plt.title(f\"RF FI (Production) - SKU{sid}\"); savefig(os.path.join(OUT_DIR, f\"fi_production_SKU{sid}_top20.png\"))\n",
    "        plt.figure(figsize=(8,6)); topk(pi_pr,20).iloc[::-1].plot(kind=\"barh\"); plt.title(f\"Permutation FI (Production) - SKU{sid}\"); savefig(os.path.join(OUT_DIR, f\"pi_production_SKU{sid}_top20.png\"))\n",
    "        pr_pack = dict(rf=(rf_pr, spl_pr, m_rf_pr), rg=(rg_pr, spl_rg_pr, m_rg_pr),\n",
    "                       fi=fi_pr, pi=pi_pr, coef=coef_pr, beta_std=beta_std_pr, feats=list(X_pr.columns))\n",
    "    else:\n",
    "        print(f\"[WARN] SKU{sid}: Production features empty → skip\")\n",
    "        pr_pack = None\n",
    "\n",
    "    per_sku_results[sid] = dict(lead_time=lt_pack, production=pr_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728947b9-fdae-48d0-ba2b-e628e059ab71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9c4630c-48d9-4741-8d37-160211895378",
   "metadata": {},
   "source": [
    "# 5) 탄력도(민감도) 정량화: PDP 기울기 + 선형계수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb42eab-ddd1-441b-b2c9-f2f31d73700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 경량 탄력도 설정 ===\n",
    "from numpy.random import default_rng\n",
    "RANDOM_STATE = 42\n",
    "_rng = default_rng(RANDOM_STATE)\n",
    "\n",
    "# 중요도 기반 후보 피처 선택 (Queue/Util 우선)\n",
    "FEATURES_TOP_K = 12          # 너무 크면 시간 증가\n",
    "ONLY_QUEUE_UTIL = True\n",
    "SAMPLE_N = 20000             # 2~5만 추천\n",
    "USE_FDP_FAST = True          # 빠른 유한차분법(권장). False면 PDP(느림)\n",
    "\n",
    "def _pick_features(X: pd.DataFrame, fi: Optional[pd.Series]) -> List[str]:\n",
    "    cols = X.columns.tolist()\n",
    "    if ONLY_QUEUE_UTIL:\n",
    "        base = [c for c in cols if c.endswith(\"_Queue\")] + [c for c in cols if c.endswith(\"_Util\")]\n",
    "        if fi is None:\n",
    "            return base[:FEATURES_TOP_K] if base else cols[:FEATURES_TOP_K]\n",
    "        ranked = [c for c in fi.index if c in base]\n",
    "        if not ranked:\n",
    "            ranked = [c for c in fi.index if c in cols]\n",
    "        return ranked[:FEATURES_TOP_K]\n",
    "    else:\n",
    "        if fi is None:\n",
    "            return cols[:FEATURES_TOP_K]\n",
    "        return [c for c in fi.index if c in cols][:FEATURES_TOP_K]\n",
    "\n",
    "ELASTICITY_FEATURES_LT = _pick_features(X_lt_agg, fi_lt_agg)\n",
    "ELASTICITY_FEATURES_PR = _pick_features(X_pr_agg, fi_pr_agg)\n",
    "print(\"ELASTICITY_FEATURES_LT:\", ELASTICITY_FEATURES_LT)\n",
    "print(\"ELASTICITY_FEATURES_PR:\", ELASTICITY_FEATURES_PR)\n",
    "\n",
    "# --- 빠른 유한차분법(FD) ---\n",
    "def _suggest_h(x: np.ndarray) -> float:\n",
    "    if x.size == 0: return 1.0\n",
    "    q25, q75 = np.percentile(x, [25, 75])\n",
    "    iqr = q75 - q25\n",
    "    if iqr > 0: return float(iqr * 0.05)\n",
    "    std = np.std(x)\n",
    "    return float(std * 0.1 if std > 0 else 1.0)\n",
    "\n",
    "def fd_slope_mean(model, X: pd.DataFrame, feature: str, h: Optional[float]=None, sample_n: int=20000) -> Optional[float]:\n",
    "    if feature not in X.columns: return None\n",
    "    n = len(X)\n",
    "    if n == 0: return None\n",
    "    if sample_n and sample_n < n:\n",
    "        idx = _rng.choice(n, size=sample_n, replace=False)\n",
    "        Xs = X.iloc[idx].copy()\n",
    "    else:\n",
    "        Xs = X.copy()\n",
    "    x = Xs[feature].to_numpy()\n",
    "    if h is None: h = _suggest_h(x)\n",
    "    if h == 0: return 0.0\n",
    "    X_minus = Xs.copy(); X_plus = Xs.copy()\n",
    "    X_minus[feature] = x - h\n",
    "    X_plus[feature]  = x + h\n",
    "    y_m = model.predict(X_minus).mean()\n",
    "    y_p = model.predict(X_plus).mean()\n",
    "    return float((y_p - y_m) / (2.0 * h))\n",
    "\n",
    "# --- (옵션) PDP 기반 — 정확하지만 느림 ---\n",
    "from sklearn.inspection import partial_dependence\n",
    "def pdp_slope_mean(model, X: pd.DataFrame, feature: str, h: Optional[float]=None, sample_n: int=20000) -> Optional[float]:\n",
    "    if feature not in X.columns: return None\n",
    "    n = len(X)\n",
    "    if n == 0: return None\n",
    "    if sample_n and sample_n < n:\n",
    "        idx = _rng.choice(n, size=sample_n, replace=False)\n",
    "        Xs = X.iloc[idx].copy()\n",
    "    else:\n",
    "        Xs = X.copy()\n",
    "    x = Xs[feature].to_numpy()\n",
    "    if h is None: h = _suggest_h(x)\n",
    "    grid = [np.median(x) - h, np.median(x), np.median(x) + h]\n",
    "    pdp = partial_dependence(model, Xs, [feature], grid=grid, kind=\"average\")\n",
    "    ys = pdp.average[0]\n",
    "    xs = np.array(grid, dtype=float)\n",
    "    denom = (xs[-1] - xs[0])\n",
    "    return float((ys[-1] - ys[0]) / denom) if denom != 0 else 0.0\n",
    "\n",
    "def run_elasticity_block(tag: str, model, X: pd.DataFrame, features: List[str],\n",
    "                         use_fast: bool=True, sample_n: int=20000) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for f in features:\n",
    "        s = fd_slope_mean(model, X, f, h=None, sample_n=sample_n) if use_fast else pdp_slope_mean(model, X, f, h=None, sample_n=sample_n)\n",
    "        rows.append({\"feature\": f, \"slope\": s})\n",
    "    out = pd.DataFrame(rows).dropna().sort_values(\"slope\", ascending=False)\n",
    "    out.to_csv(os.path.join(OUT_DIR, f\"elasticity_{tag}.csv\"), index=False)\n",
    "    return out\n",
    "\n",
    "elasticity_outputs = {}\n",
    "\n",
    "# AGG LeadTime\n",
    "el_lt = run_elasticity_block(\n",
    "    tag=\"leadtime_AGG_fast\" if USE_FDP_FAST else \"leadtime_AGG_pdp\",\n",
    "    model=rf_lt_agg, X=X_lt_agg, features=ELASTICITY_FEATURES_LT,\n",
    "    use_fast=USE_FDP_FAST, sample_n=SAMPLE_N\n",
    ")\n",
    "elasticity_outputs[\"leadtime_AGG\"] = el_lt\n",
    "display(el_lt.head(15))\n",
    "\n",
    "# AGG Production\n",
    "el_pr = run_elasticity_block(\n",
    "    tag=\"production_AGG_fast\" if USE_FDP_FAST else \"production_AGG_pdp\",\n",
    "    model=rf_pr_agg, X=X_pr_agg, features=ELASTICITY_FEATURES_PR,\n",
    "    use_fast=USE_FDP_FAST, sample_n=SAMPLE_N\n",
    ")\n",
    "elasticity_outputs[\"production_AGG\"] = el_pr\n",
    "display(el_pr.head(15))\n",
    "\n",
    "# (선택) SKU별에도 동일 적용\n",
    "per_sku_elasticity = {}\n",
    "for sid, packs in per_sku_results.items():\n",
    "    per_sku_elasticity[sid] = {}\n",
    "    # LeadTime\n",
    "    if packs.get(\"lead_time\"):\n",
    "        rf = packs[\"lead_time\"][\"rf\"][0]\n",
    "        Xte = packs[\"lead_time\"][\"rf\"][1][1]\n",
    "        feats = [f for f in ELASTICITY_FEATURES_LT if f in packs[\"lead_time\"][\"feats\"]]\n",
    "        if feats:\n",
    "            tag = f\"leadtime_SKU{sid}_\" + (\"fast\" if USE_FDP_FAST else \"pdp\")\n",
    "            out = run_elasticity_block(tag, rf, Xte, feats, USE_FDP_FAST, SAMPLE_N)\n",
    "            per_sku_elasticity[sid][\"lead_time\"] = out\n",
    "    # Production\n",
    "    if packs.get(\"production\"):\n",
    "        rf = packs[\"production\"][\"rf\"][0]\n",
    "        Xte = packs[\"production\"][\"rf\"][1][1]\n",
    "        feats = [f for f in ELASTICITY_FEATURES_PR if f in packs[\"production\"][\"feats\"]]\n",
    "        if feats:\n",
    "            tag = f\"production_SKU{sid}_\" + (\"fast\" if USE_FDP_FAST else \"pdp\")\n",
    "            out = run_elasticity_block(tag, rf, Xte, feats, USE_FDP_FAST, SAMPLE_N)\n",
    "            per_sku_elasticity[sid][\"production\"] = out\n",
    "\n",
    "print(\"Elasticity CSVs saved to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28508184-967b-40a7-b9f9-4e2d4008f112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dce9bb5-56dc-4b1a-af9e-047d23f58390",
   "metadata": {},
   "source": [
    "# 6) What‑if 시뮬레이터 (AGG & SKU별)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe373733-c87a-4de6-8156-5a85255ba0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_delta(model, X_base: pd.DataFrame, deltas: Dict[str, float]) -> Tuple[float, float, float]:\n",
    "    X_sim = X_base.copy()\n",
    "    for k, v in deltas.items():\n",
    "        if k in X_sim.columns:\n",
    "            X_sim[k] = X_sim[k] + v\n",
    "    base_mean = float(model.predict(X_base).mean())\n",
    "    new_mean  = float(model.predict(X_sim).mean())\n",
    "    return base_mean, new_mean, new_mean - base_mean\n",
    "\n",
    "SCENARIOS = {\n",
    "    \"BlankingQ_-50\": {\"Blanking_Queue\": -50.0} if \"Blanking_Queue\" in X_lt_agg.columns else {},\n",
    "    \"Warehouse1Q_-50\": {\"Warehouse1_Queue\": -50.0} if \"Warehouse1_Queue\" in X_lt_agg.columns else {},\n",
    "    \"QualityUtil_+5\": {\"Quality_Util\": +5.0} if \"Quality_Util\" in X_lt_agg.columns else {},\n",
    "}\n",
    "\n",
    "whatif = {\"AGG\": {\"lead_time\": {}, \"production\": {}}, \"SKU\": {}}\n",
    "\n",
    "# AGG LT\n",
    "Xb_lt = splits_lt_agg[1]\n",
    "for name, d in SCENARIOS.items():\n",
    "    if not d: continue\n",
    "    base_m, new_m, d_m = simulate_delta(rf_lt_agg, Xb_lt, d)\n",
    "    whatif[\"AGG\"][\"lead_time\"][name] = {\"base_mean\": base_m, \"new_mean\": new_m, \"delta_mean\": d_m}\n",
    "\n",
    "# AGG Production\n",
    "Xb_pr = splits_pr_agg[1]\n",
    "for name, d in SCENARIOS.items():\n",
    "    if not d: continue\n",
    "    base_m, new_m, d_m = simulate_delta(rf_pr_agg, Xb_pr, d)\n",
    "    whatif[\"AGG\"][\"production\"][name] = {\"base_mean\": base_m, \"new_mean\": new_m, \"delta_mean\": d_m}\n",
    "\n",
    "# SKU별\n",
    "for sid, packs in per_sku_results.items():\n",
    "    whatif[\"SKU\"][sid] = {}\n",
    "    if packs[\"lead_time\"] is not None:\n",
    "        rf, (Xtr, Xte, ytr, yte), _ = packs[\"lead_time\"][\"rf\"]\n",
    "        feats = packs[\"lead_time\"][\"feats\"]\n",
    "        lt_map = {}\n",
    "        for name, d in SCENARIOS.items():\n",
    "            d_sub = {k:v for k,v in d.items() if k in feats}\n",
    "            if not d_sub: continue\n",
    "            base_m, new_m, d_m = simulate_delta(rf, Xte[feats], d_sub)\n",
    "            lt_map[name] = {\"base_mean\": base_m, \"new_mean\": new_m, \"delta_mean\": d_m}\n",
    "        whatif[\"SKU\"][sid][\"lead_time\"] = lt_map\n",
    "    if packs[\"production\"] is not None:\n",
    "        rf, (Xtr, Xte, ytr, yte), _ = packs[\"production\"][\"rf\"]\n",
    "        feats = packs[\"production\"][\"feats\"]\n",
    "        pr_map = {}\n",
    "        for name, d in SCENARIOS.items():\n",
    "            d_sub = {k:v for k,v in d.items() if k in feats}\n",
    "            if not d_sub: continue\n",
    "            base_m, new_m, d_m = simulate_delta(rf, Xte[feats], d_sub)\n",
    "            pr_map[name] = {\"base_mean\": base_m, \"new_mean\": new_m, \"delta_mean\": d_m}\n",
    "        whatif[\"SKU\"][sid][\"production\"] = pr_map\n",
    "\n",
    "with open(ensure_dir(os.path.join(OUT_DIR, \"whatif_results.json\")), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(whatif, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"=== What-if (AGG) ===\")\n",
    "print(json.dumps(whatif[\"AGG\"], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba1566-6add-45ca-af71-cc355890bc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfd665fd-48db-4c48-a6ea-f4f4cc705858",
   "metadata": {},
   "source": [
    "# 7) (옵션) 결함/불량 타깃 자동 탐지 · 동일 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac70c05-f5d8-4cb6-a0e0-3f9ee12726cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFECT_PAT = re.compile(r\"(defect|scrap|reject|ng|fail)\", re.IGNORECASE)\n",
    "defect_targets = [c for c in df.columns if DEFECT_PAT.search(c)]\n",
    "\n",
    "if defect_targets:\n",
    "    print(\"=== Detected defect targets ===\", defect_targets)\n",
    "    for tgt in defect_targets:\n",
    "        y_df = pd.to_numeric(df[tgt], errors=\"coerce\").fillna(0.0).astype(\"float32\").values\n",
    "        X_df = build_features_for_leadtime(df, exclude_cols=[], add_bneck_onehot=True, top_bneck_list=TOP_BNECKS)\n",
    "        rf, spl, m_rf, fi, pi = train_rf(X_df, y_df)\n",
    "        rg, spl2, m_rg, coef, beta_std = train_ridge(X_df, y_df)\n",
    "        base = tgt.replace(os.sep, \"_\")\n",
    "        fi.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"fi_defect_{base}.csv\")))\n",
    "        pi.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"pi_defect_{base}.csv\")))\n",
    "        coef.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"ridge_coef_defect_{base}.csv\")))\n",
    "        beta_std.to_csv(ensure_dir(os.path.join(OUT_DIR, f\"ridge_betaStd_defect_{base}.csv\")))\n",
    "        print(f\"[{tgt}] RF={m_rf}, Ridge={m_rg}\")\n",
    "else:\n",
    "    print(\"No defect/scrap targets detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace1c56-db4f-4762-b4d2-fbbd2edc3bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a4e4eca-4135-45e3-ba36-863b907e3da4",
   "metadata": {},
   "source": [
    "# 8) (옵션) mu/sd 기반 편차 파생피처 추가 – 필요 시만 ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4227d187-9385-4730-9e98-1908cacb88a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_MUSD_IN_LT = False   # 리드타임 모델 포함 여부\n",
    "INCLUDE_MUSD_IN_PR = True    # 생산량 모델 포함 여부\n",
    "MUSD: Dict[str, Dict[str, Dict[str, float]]] = {\n",
    "    \"SKU1\": {\n",
    "        \"mu\": {\"BL\":900.0, \"PR\":5.0, \"AS\":25.0, \"PA\":5400.0, \"QL\":55.0},\n",
    "        \"sd\": {\"BL\":30.0,  \"PR\":0.1, \"AS\":0.1,  \"PA\":0.0,    \"QL\":2.04},\n",
    "    }\n",
    "}\n",
    "REPLICATE_TO_OTHERS = True\n",
    "\n",
    "def add_musd_features(df_: pd.DataFrame, sku_ids_: List[str]) -> List[str]:\n",
    "    created = []\n",
    "    for sid in sku_ids_:\n",
    "        key = f\"SKU{sid}\"\n",
    "        if key not in MUSD and REPLICATE_TO_OTHERS and \"SKU1\" in MUSD:\n",
    "            MUSD[key] = MUSD[\"SKU1\"]\n",
    "        if key not in MUSD:\n",
    "            continue\n",
    "        mu = MUSD[key][\"mu\"]; sd = MUSD[key][\"sd\"]\n",
    "        mu_sum = mu[\"BL\"] + mu[\"PR\"] + mu[\"AS\"] + mu[\"PA\"] + mu[\"QL\"]\n",
    "        S_var  = sd[\"BL\"]**2 + sd[\"PR\"]**2 + sd[\"AS\"]**2 + sd[\"QL\"]**2\n",
    "        S_std  = math.sqrt(S_var) if S_var > 0 else 1.0\n",
    "        base = f\"SKU{sid}\"\n",
    "        va_col = next((base+s for s in LT_PATTERNS[\"VA\"] if (base+s) in df_.columns), None)\n",
    "        VA = pd.to_numeric(df_[va_col], errors=\"coerce\").fillna(0.0) if va_col else pd.Series(0.0, index=df_.index)\n",
    "        dev_col = f\"{base}_VA_dev\"; z_col = f\"{base}_VA_z\"\n",
    "        df_[dev_col] = (VA - mu_sum).astype(\"float32\")\n",
    "        df_[z_col]   = ((VA - mu_sum)/S_std).astype(\"float32\")\n",
    "        created += [dev_col, z_col]\n",
    "    return created\n",
    "\n",
    "# 사용하려면:\n",
    "# musd_cols = add_musd_features(df, sku_ids)\n",
    "# print(\"MUSD features created:\", musd_cols)\n",
    "# -> 이후 '셀 4' 모델 재학습 시 생산량 모델에는 포함(리드타임 모델은 기본 제외)하도록 응용 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c9e31-e8f8-4272-921c-ba2e054ee78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89466bff-f886-4e64-bdf2-bd5c8c52514f",
   "metadata": {},
   "source": [
    "# 9) 요약 리포트 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174fc6b-70e6-4970-8395-2c05f62de8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"cwd\": os.getcwd(),\n",
    "    \"dataset_shape\": tuple(df.shape),\n",
    "    \"detected_skus\": sku_ids,\n",
    "    \"top_bottlenecks_used\": TOP_BNECKS,\n",
    "    \"metrics\": {\n",
    "        \"AGG\": {\n",
    "            \"RF_LT\": m_rf_lt_agg, \"RG_LT\": m_rg_lt_agg,\n",
    "            \"RF_PR\": m_rf_pr_agg, \"RG_PR\": m_rg_pr_agg,\n",
    "        }\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"bottleneck_total\": os.path.join(OUT_DIR, \"bottleneck_totalProduction.csv\"),\n",
    "        \"bottleneck_aggLT\": os.path.join(OUT_DIR, \"bottleneck_aggLeadTime.csv\"),\n",
    "        \"sku_bneck_effect\": \"sku_bneck_effect_SKU*.csv\",\n",
    "        \"agg_fi_pi_lt\": [\"fi_leadtime_AGG.csv\",\"pi_leadtime_AGG.csv\",\"fi_leadtime_AGG_top20.png\",\n",
    "                         \"ridge_coef_leadtime_AGG.csv\",\"ridge_betaStd_leadtime_AGG.csv\",\n",
    "                         \"elasticity_leadtime_AGG_fast.csv\"],\n",
    "        \"agg_fi_pi_pr\": [\"fi_production_AGG.csv\",\"pi_production_AGG.csv\",\"fi_production_AGG_top20.png\",\n",
    "                         \"ridge_coef_production_AGG.csv\",\"ridge_betaStd_production_AGG.csv\",\n",
    "                         \"elasticity_production_AGG_fast.csv\"],\n",
    "        \"per_sku_elasticity\": \"elasticity_(leadtime|production)_SKU*_fast.csv\",\n",
    "        \"whatif\": os.path.join(OUT_DIR, \"whatif_results.json\")\n",
    "    },\n",
    "    \"notes\": [\n",
    "        \"생산량 모델에서 c_TotalProducts, c_Cell* 피처 제외 → 타깃 누수 방지.\",\n",
    "        \"리드타임 모델에서 *_Time/_sec/_LeadTime/_Total 피처 제외 → 누수 방지.\",\n",
    "        \"병목 라벨(_bneck) 원-핫 추가로 병목 상태의 평균적 영향 반영.\",\n",
    "        \"탄력도: FD(샘플링)으로 빠르게 ∂ŷ/∂x 추정, 필요시 PDP 대체 가능.\",\n",
    "    ]\n",
    "}\n",
    "with open(ensure_dir(os.path.join(OUT_DIR, \"SUMMARY.json\")), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(json.dumps(summary, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e385eaa-8ac6-4cef-a925-50ad0870ea86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a54010-6243-4b16-9bb1-a3ca35a3f0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea47372-d0d0-47e7-9914-157d4db0faa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
