{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8db895-6d49-46f1-aef6-55683438412a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1380b32-463d-4ed5-9f21-d8dd8c6f79ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilter: 22 columns dropped. Report → outputs/prefilter_report.csv\n",
      "\n",
      "============================================================\n",
      "Running scenario: All\n",
      "============================================================\n",
      "\n",
      "[Scenario: All] ▶ Training LinearRegression ...\n",
      "[OK] LinearRegression: test_R2=0.9996\n",
      "\n",
      "[Scenario: All] ▶ Training RidgeCV ...\n",
      "[OK] RidgeCV: test_R2=0.9993\n",
      "\n",
      "[Scenario: All] ▶ Training ElasticNetCV ...\n",
      "[OK] ElasticNetCV: test_R2=0.9991\n",
      "\n",
      "[Scenario: All] ▶ Training RandomForest ...\n",
      "[OK] RandomForest: test_R2=1.0000\n",
      "\n",
      "[Scenario: All] ▶ Training SkMLP(epochs=50) ...\n",
      "[OK] SkMLP(epochs=50): test_R2=0.9998\n",
      "[DONE] Scenario 'All' — outputs at: outputs\\scenarios\\All\n",
      "\n",
      "============================================================\n",
      "Running scenario: NoQuality\n",
      "============================================================\n",
      "\n",
      "[Scenario: NoQuality] ▶ Training LinearRegression ...\n",
      "[OK] LinearRegression: test_R2=0.9993\n",
      "\n",
      "[Scenario: NoQuality] ▶ Training RidgeCV ...\n",
      "[OK] RidgeCV: test_R2=0.9993\n",
      "\n",
      "[Scenario: NoQuality] ▶ Training ElasticNetCV ...\n",
      "[OK] ElasticNetCV: test_R2=0.9993\n",
      "\n",
      "[Scenario: NoQuality] ▶ Training RandomForest ...\n",
      "[OK] RandomForest: test_R2=0.9992\n",
      "\n",
      "[Scenario: NoQuality] ▶ Training SkMLP(epochs=50) ...\n",
      "[OK] SkMLP(epochs=50): test_R2=0.9994\n",
      "[DONE] Scenario 'NoQuality' — outputs at: outputs\\scenarios\\NoQuality\n",
      "\n",
      "============================================================\n",
      "Running scenario: NoQualityPaint\n",
      "============================================================\n",
      "\n",
      "[Scenario: NoQualityPaint] ▶ Training LinearRegression ...\n",
      "[OK] LinearRegression: test_R2=0.9992\n",
      "\n",
      "[Scenario: NoQualityPaint] ▶ Training RidgeCV ...\n",
      "[OK] RidgeCV: test_R2=0.9992\n",
      "\n",
      "[Scenario: NoQualityPaint] ▶ Training ElasticNetCV ...\n",
      "[OK] ElasticNetCV: test_R2=0.9992\n",
      "\n",
      "[Scenario: NoQualityPaint] ▶ Training RandomForest ...\n",
      "[OK] RandomForest: test_R2=0.9985\n",
      "\n",
      "[Scenario: NoQualityPaint] ▶ Training SkMLP(epochs=50) ...\n",
      "[OK] SkMLP(epochs=50): test_R2=0.9993\n",
      "[DONE] Scenario 'NoQualityPaint' — outputs at: outputs\\scenarios\\NoQualityPaint\n",
      "\n",
      "============================================================\n",
      "Running scenario: NoQualityPaintAssembly\n",
      "============================================================\n",
      "\n",
      "[Scenario: NoQualityPaintAssembly] ▶ Training LinearRegression ...\n",
      "[OK] LinearRegression: test_R2=0.9989\n",
      "\n",
      "[Scenario: NoQualityPaintAssembly] ▶ Training RidgeCV ...\n",
      "[OK] RidgeCV: test_R2=0.9989\n",
      "\n",
      "[Scenario: NoQualityPaintAssembly] ▶ Training ElasticNetCV ...\n",
      "[OK] ElasticNetCV: test_R2=0.9989\n",
      "\n",
      "[Scenario: NoQualityPaintAssembly] ▶ Training RandomForest ...\n",
      "[OK] RandomForest: test_R2=0.9979\n",
      "\n",
      "[Scenario: NoQualityPaintAssembly] ▶ Training SkMLP(epochs=50) ...\n",
      "[OK] SkMLP(epochs=50): test_R2=0.9989\n",
      "[DONE] Scenario 'NoQualityPaintAssembly' — outputs at: outputs\\scenarios\\NoQualityPaintAssembly\n",
      "\n",
      "============================================================\n",
      "Running scenario: BlankingPressOnly\n",
      "============================================================\n",
      "\n",
      "[Scenario: BlankingPressOnly] ▶ Training LinearRegression ...\n",
      "[OK] LinearRegression: test_R2=0.9988\n",
      "\n",
      "[Scenario: BlankingPressOnly] ▶ Training RidgeCV ...\n",
      "[OK] RidgeCV: test_R2=0.9988\n",
      "\n",
      "[Scenario: BlankingPressOnly] ▶ Training ElasticNetCV ...\n",
      "[OK] ElasticNetCV: test_R2=0.9988\n",
      "\n",
      "[Scenario: BlankingPressOnly] ▶ Training RandomForest ...\n",
      "[OK] RandomForest: test_R2=0.9978\n",
      "\n",
      "[Scenario: BlankingPressOnly] ▶ Training SkMLP(epochs=50) ...\n",
      "[OK] SkMLP(epochs=50): test_R2=0.9988\n",
      "[DONE] Scenario 'BlankingPressOnly' — outputs at: outputs\\scenarios\\BlankingPressOnly\n",
      "\n",
      "============================================================\n",
      "Running scenario: BlankingOnly\n",
      "============================================================\n",
      "\n",
      "[Scenario: BlankingOnly] ▶ Training LinearRegression ...\n",
      "[OK] LinearRegression: test_R2=0.9732\n",
      "\n",
      "[Scenario: BlankingOnly] ▶ Training RidgeCV ...\n",
      "[OK] RidgeCV: test_R2=0.9732\n",
      "\n",
      "[Scenario: BlankingOnly] ▶ Training ElasticNetCV ...\n",
      "[OK] ElasticNetCV: test_R2=0.9732\n",
      "\n",
      "[Scenario: BlankingOnly] ▶ Training RandomForest ...\n",
      "[OK] RandomForest: test_R2=0.9959\n",
      "\n",
      "[Scenario: BlankingOnly] ▶ Training SkMLP(epochs=50) ...\n",
      "[OK] SkMLP(epochs=50): test_R2=0.9955\n",
      "[DONE] Scenario 'BlankingOnly' — outputs at: outputs\\scenarios\\BlankingOnly\n",
      "\n",
      "All scenarios done. Check 'outputs/scenarios/<name>/' folders.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Production Quantity Modeling — Multi-Scenario by Process Stages (FINAL+SAVES)\n",
    "- 요구사항: 시나리오별 학습/평가 + 모든 산출물 저장\n",
    "\"\"\"\n",
    "\n",
    "import os, re, json, warnings, inspect\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import joblib\n",
    "\n",
    "# ===== Optional boosters =====\n",
    "have_xgb = have_lgbm = have_cat = False\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    have_xgb = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    have_lgbm = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    have_cat = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ===== PyTorch (AI 모델 1개) =====\n",
    "have_torch = False\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    have_torch = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# =====================\n",
    "# 0) CONFIG\n",
    "# =====================\n",
    "FILE_PATH   = r\"Final Results Extended.csv\"   # ← 네 CSV 경로로 수정\n",
    "TARGET_COL  = \"c_TotalProducts\"\n",
    "RANDOM_SEED = 42\n",
    "OUT_DIR     = \"outputs\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
    "PROGRESS_PERIOD = 50  # booster 로그 주기\n",
    "\n",
    "# Prefilter thresholds\n",
    "DROP_MISSING_PCT = 0.95\n",
    "DROP_ZERO_PCT    = 0.95\n",
    "DROP_NZV_VAR     = 1e-8\n",
    "\n",
    "def log(*args, **kwargs):\n",
    "    kwargs.setdefault('flush', True)\n",
    "    print(*args, **kwargs)\n",
    "\n",
    "def save_fig(path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# =====================\n",
    "# 1) LOAD & BASIC CLEAN\n",
    "# =====================\n",
    "df = pd.read_csv(FILE_PATH, low_memory=False)\n",
    "df.columns = [re.sub(r\"__+\", \"_\", c.strip()) for c in df.columns]\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not num_cols:\n",
    "        raise ValueError(\"No numeric columns and target not found.\")\n",
    "    TARGET_COL = df[num_cols].var().sort_values(ascending=False).index[0]\n",
    "    print(f\"[WARN] '{TARGET_COL}'을(를) 타깃으로 가정합니다. 실제 타깃을 설정하세요.\")\n",
    "\n",
    "# =====================\n",
    "# 2) DATETIME DETECTION\n",
    "# =====================\n",
    "def is_datetime_like(s: pd.Series, sample:int=500) -> bool:\n",
    "    vals = s.dropna().astype(str).head(sample)\n",
    "    if len(vals) == 0: return False\n",
    "    parsed = pd.to_datetime(vals, errors=\"coerce\", infer_datetime_format=True)\n",
    "    return parsed.notna().mean() >= 0.8\n",
    "\n",
    "dt_col: Optional[str] = None\n",
    "for c in df.columns:\n",
    "    if c == TARGET_COL: continue\n",
    "    hint = bool(re.search(r\"(date|time|timestamp)\", c, re.I))\n",
    "    if hint or df[c].dtype == \"object\":\n",
    "        if is_datetime_like(df[c]):\n",
    "            dt_col = c\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
    "            break\n",
    "\n",
    "if dt_col is not None:\n",
    "    parsed = pd.to_datetime(df[dt_col], errors=\"coerce\")\n",
    "    df[\"year\"]      = parsed.dt.year\n",
    "    df[\"month\"]     = parsed.dt.month\n",
    "    df[\"day\"]       = parsed.dt.day\n",
    "    df[\"dayofweek\"] = parsed.dt.dayofweek\n",
    "\n",
    "# =====================\n",
    "# 3) PREFILTER\n",
    "# =====================\n",
    "def prefilter_features(frame: pd.DataFrame, target: str):\n",
    "    feats = frame.drop(columns=[target])\n",
    "    report, drop = [], set()\n",
    "\n",
    "    num_cols = feats.select_dtypes(include=[np.number]).columns\n",
    "    cat_cols = feats.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # Numeric\n",
    "    for c in num_cols:\n",
    "        s = pd.to_numeric(feats[c], errors='coerce')\n",
    "        n = len(s)\n",
    "        miss = float(s.isna().mean())\n",
    "        zero = float((s.fillna(0) == 0).mean())\n",
    "        uniq = int(s.nunique(dropna=True))\n",
    "        var  = float(s.var()) if n > 1 else 0.0\n",
    "        sug = (miss >= DROP_MISSING_PCT) or (uniq <= 1) or (zero >= DROP_ZERO_PCT) or (var <= DROP_NZV_VAR)\n",
    "        if sug: drop.add(c)\n",
    "        report.append({'column': c,'dtype':'numeric','missing_pct':miss,'zero_pct':zero,'unique':uniq,'variance':var,'suggest_drop':bool(sug)})\n",
    "\n",
    "    # Categorical\n",
    "    for c in cat_cols:\n",
    "        s = feats[c].astype('object')\n",
    "        miss = float(s.isna().mean())\n",
    "        uniq = int(s.nunique(dropna=True))\n",
    "        sug = (miss >= DROP_MISSING_PCT) or (uniq <= 1)\n",
    "        if sug: drop.add(c)\n",
    "        report.append({'column': c,'dtype':'categorical','missing_pct':miss,'zero_pct':np.nan,'unique':uniq,'variance':np.nan,'suggest_drop':bool(sug)})\n",
    "\n",
    "    rep_df = pd.DataFrame(report).sort_values(['suggest_drop','missing_pct'], ascending=[False, False])\n",
    "    rep_df.to_csv(os.path.join(OUT_DIR, 'prefilter_report.csv'), index=False)\n",
    "    print(f\"Prefilter: {len(drop)} columns dropped. Report → outputs/prefilter_report.csv\")\n",
    "\n",
    "    return feats.drop(columns=list(drop)), list(drop)\n",
    "\n",
    "X_prefilt, dropped_cols = prefilter_features(df, TARGET_COL)\n",
    "y_all = pd.to_numeric(df[TARGET_COL], errors=\"coerce\")\n",
    "X_all = X_prefilt.copy()\n",
    "\n",
    "# 상수/빈 컬럼 제거\n",
    "bad_cols = [c for c in X_all.columns if X_all[c].isna().all() or X_all[c].nunique(dropna=True) <= 1]\n",
    "if bad_cols:\n",
    "    X_all = X_all.drop(columns=bad_cols)\n",
    "    dropped_cols.extend(bad_cols)\n",
    "\n",
    "# =====================\n",
    "# 3.5) LEAKAGE SCAN\n",
    "# =====================\n",
    "AUTO_DROP_LEAKS   = True\n",
    "LEAK_CORR_THRESH  = 0.999\n",
    "EQUAL_TOL         = 1e-9\n",
    "\n",
    "def leakage_scan(Xdf: pd.DataFrame, yser: pd.Series):\n",
    "    rows = []\n",
    "    yv = pd.to_numeric(yser, errors='coerce')\n",
    "    for c in Xdf.columns:\n",
    "        s = Xdf[c]\n",
    "        equal_flag = False\n",
    "        corr_val = np.nan\n",
    "        try:\n",
    "            diff = (pd.to_numeric(s, errors='coerce') - yv).abs()\n",
    "            equal_flag = bool(np.nanmax(diff.values) <= EQUAL_TOL)\n",
    "        except Exception:\n",
    "            equal_flag = False\n",
    "        try:\n",
    "            s_num = pd.to_numeric(s, errors='coerce')\n",
    "            if s_num.notna().sum() > 3 and yv.notna().sum() > 3:\n",
    "                corr_val = float(np.corrcoef(s_num.fillna(s_num.median()), yv.fillna(yv.median()))[0,1])\n",
    "        except Exception:\n",
    "            corr_val = np.nan\n",
    "        rows.append({'column': c, 'equal_to_target': equal_flag, 'pearson_corr_to_target': corr_val})\n",
    "    rep = pd.DataFrame(rows)\n",
    "    rep['abs_corr'] = rep['pearson_corr_to_target'].abs()\n",
    "    rep['suspect'] = rep['equal_to_target'] | (rep['abs_corr'] >= LEAK_CORR_THRESH)\n",
    "    rep.sort_values(['suspect','abs_corr'], ascending=[False, False]).to_csv(os.path.join(OUT_DIR, 'leakage_report_overall.csv'), index=False)\n",
    "    suspects = rep.loc[rep['suspect'], 'column'].tolist()\n",
    "    return suspects\n",
    "\n",
    "overall_suspects = leakage_scan(X_all, y_all)\n",
    "\n",
    "# =====================\n",
    "# 4) PROCESS STAGES & SCENARIOS\n",
    "# =====================\n",
    "STAGE_PATTERNS: Dict[str, List[str]] = {\n",
    "    \"blanking\":       [r\"^Blanking\", r\"\\bBlanking\\b\"],\n",
    "    \"forklift_b\":     [r\"Forklift_Blan?king\", r\"Forklift.*Blank\"],\n",
    "    \"press\":          [r\"^Press[1-4]_\"],\n",
    "    \"forklift_p\":     [r\"Forklift_Press\"],\n",
    "    \"warehouse\":      [r\"^Warehouse[1-4]_\"],\n",
    "    \"assembly_cell\":  [r\"^Cell[1-4]_\", r\"^c_Cell\", r\"^c_Cycle\", r\"^c_.*SKU\", r\"^SKU[1-4]_\"],\n",
    "    \"forklift_a\":     [r\"Forklift_Assembly\"],\n",
    "    \"paint\":          [r\"^Paint[12]_\"],\n",
    "    \"quality\":        [r\"^Quality_\"]\n",
    "}\n",
    "\n",
    "def cols_of_stages(cols: List[str], stages: List[str]) -> List[str]:\n",
    "    picked = []\n",
    "    for st in stages:\n",
    "        pats = STAGE_PATTERNS.get(st, [])\n",
    "        for p in pats:\n",
    "            rgx = re.compile(p, re.I)\n",
    "            picked.extend([c for c in cols if rgx.search(c)])\n",
    "    return sorted(list(dict.fromkeys(picked)))\n",
    "\n",
    "ALL_STAGES = [\"blanking\",\"forklift_b\",\"press\",\"forklift_p\",\"warehouse\",\"assembly_cell\",\"forklift_a\",\"paint\",\"quality\"]\n",
    "\n",
    "SCENARIOS = [\n",
    "    {\"name\": \"All\",                    \"include\": ALL_STAGES,                                                                                 \"exclude\": []},\n",
    "    {\"name\": \"NoQuality\",              \"include\": [s for s in ALL_STAGES if s != \"quality\"],                                                  \"exclude\": []},\n",
    "    {\"name\": \"NoQualityPaint\",         \"include\": [s for s in ALL_STAGES if s not in {\"quality\",\"paint\"}],                                    \"exclude\": []},\n",
    "    {\"name\": \"NoQualityPaintAssembly\", \"include\": [s for s in ALL_STAGES if s not in {\"quality\",\"paint\",\"assembly_cell\"}],                    \"exclude\": []},\n",
    "    {\"name\": \"BlankingPressOnly\",      \"include\": [\"blanking\",\"press\"],                                                                       \"exclude\": []},\n",
    "    {\"name\": \"BlankingOnly\",           \"include\": [\"blanking\"],                                                                               \"exclude\": []},\n",
    "]\n",
    "\n",
    "# =====================\n",
    "# 5) PREPROCESSORS\n",
    "# =====================\n",
    "def make_preprocessors(X: pd.DataFrame):\n",
    "    num_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "    num_tf_plain   = Pipeline([('imp', SimpleImputer(strategy='median'))])\n",
    "    num_tf_scaled  = Pipeline([('imp', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "\n",
    "    # OneHot: 버전 호환\n",
    "    try:\n",
    "        cat_ohe = OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=0.01, sparse_output=False)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            cat_ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        except TypeError:\n",
    "            cat_ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    cat_tf = Pipeline([('imp', SimpleImputer(strategy='most_frequent')), ('ohe', cat_ohe)])\n",
    "\n",
    "    transformers = []\n",
    "    if len(num_features) > 0:\n",
    "        transformers.append(('num', num_tf_plain, num_features))\n",
    "    if len(cat_features) > 0:\n",
    "        transformers.append(('cat', cat_tf, cat_features))\n",
    "    preprocess_plain  = ColumnTransformer(transformers, remainder='drop')\n",
    "\n",
    "    transformers_s = []\n",
    "    if len(num_features) > 0:\n",
    "        transformers_s.append(('num', num_tf_scaled, num_features))\n",
    "    if len(cat_features) > 0:\n",
    "        transformers_s.append(('cat', cat_tf, cat_features))\n",
    "    preprocess_scaled = ColumnTransformer(transformers_s, remainder='drop')\n",
    "\n",
    "    return preprocess_plain, preprocess_scaled, num_features, cat_features\n",
    "\n",
    "# =====================\n",
    "# 6) MODEL ZOO (+ Torch MLP)\n",
    "# =====================\n",
    "seed = RANDOM_SEED\n",
    "def get_models():\n",
    "    models: Dict[str, object] = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'RidgeCV': RidgeCV(alphas=np.logspace(-3,3,20)),\n",
    "        'ElasticNetCV': ElasticNetCV(l1_ratio=[0.1,0.5,0.9], alphas=np.logspace(-3,1,10), max_iter=5000, cv=3, random_state=seed),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=300, random_state=seed, n_jobs=-1)\n",
    "    }\n",
    "    if have_xgb:\n",
    "        models['XGBoost'] = XGBRegressor(\n",
    "            n_estimators=5000, learning_rate=0.05, max_depth=6,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            tree_method='gpu_hist', predictor='gpu_predictor',\n",
    "            n_jobs=-1, random_state=seed\n",
    "        )\n",
    "    if have_lgbm:\n",
    "        models['LightGBM'] = LGBMRegressor(n_estimators=5000, learning_rate=0.05,\n",
    "                                           num_leaves=31, subsample=0.8, colsample_bytree=0.8,\n",
    "                                           random_state=seed)\n",
    "    if have_cat:\n",
    "        models['CatBoost'] = CatBoostRegressor(iterations=5000, depth=6, learning_rate=0.05,\n",
    "                                               loss_function='RMSE', random_seed=seed,\n",
    "                                               verbose=PROGRESS_PERIOD, od_type='Iter',\n",
    "                                               od_wait=100, use_best_model=True)\n",
    "    # ML 메인 유지: sklearn MLP\n",
    "    models['SkMLP(epochs=50)'] = MLPRegressor(hidden_layer_sizes=(128,64), activation='relu', solver='adam',\n",
    "                                              max_iter=50, early_stopping=True, validation_fraction=0.15,\n",
    "                                              n_iter_no_change=10, random_state=seed, verbose=False)\n",
    "    return models\n",
    "\n",
    "NEED_SCALING = {'LinearRegression','RidgeCV','ElasticNetCV','SkMLP(epochs=50)'}\n",
    "\n",
    "def build_pipe(model_name: str, estimator, preprocess_plain, preprocess_scaled):\n",
    "    prep = preprocess_scaled if model_name in NEED_SCALING else preprocess_plain\n",
    "    return Pipeline([('prep', prep), ('model', estimator)]), prep\n",
    "\n",
    "def evaluate(y_true, y_pred) -> Tuple[float,float,float]:\n",
    "    mae = float(mean_absolute_error(y_true, y_pred))\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    r2 = float(r2_score(y_true, y_pred))\n",
    "    return mae, rmse, r2\n",
    "\n",
    "def fit_accepts(estimator, param: str) -> bool:\n",
    "    try:\n",
    "        sig = inspect.signature(estimator.fit)\n",
    "        return param in sig.parameters\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ---------- Torch MLP ----------\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(1)\n",
    "\n",
    "def prep_numeric_for_torch(X_train, X_valid, X_test, cols):\n",
    "    \"\"\"Torch 전용: 수치열만 골라 Inf→NaN → median 채움 → 표준화.\"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    Xt = X_train[cols].copy()\n",
    "    Xv = X_valid[cols].copy()\n",
    "    Xs = X_test[cols].copy()\n",
    "\n",
    "    for df_ in (Xt, Xv, Xs):\n",
    "        df_.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    med = Xt.median(numeric_only=True)\n",
    "    Xt = Xt.fillna(med); Xv = Xv.fillna(med); Xs = Xs.fillna(med)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xt_n = scaler.fit_transform(Xt)\n",
    "    Xv_n = scaler.transform(Xv)\n",
    "    Xs_n = scaler.transform(Xs)\n",
    "\n",
    "    return (pd.DataFrame(Xt_n, index=X_train.index),\n",
    "            pd.DataFrame(Xv_n, index=X_valid.index),\n",
    "            pd.DataFrame(Xs_n, index=X_test.index),\n",
    "            scaler, list(cols))\n",
    "\n",
    "def train_torch_mlp(X_train, y_train, X_valid, y_valid, scenario_dir, max_epochs=50, lr=1e-3, batch=2048, seed=RANDOM_SEED):\n",
    "    if not have_torch:\n",
    "        return None, {}\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Xtr = torch.tensor(X_train.astype(np.float32).values, device=device)\n",
    "    ytr = torch.tensor(y_train.values.astype(np.float32), device=device)\n",
    "    Xva = torch.tensor(X_valid.astype(np.float32).values, device=device)\n",
    "    yva = torch.tensor(y_valid.values.astype(np.float32), device=device)\n",
    "\n",
    "    model = TorchMLP(Xtr.shape[1]).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_state = None\n",
    "    best_rmse = float('inf')\n",
    "    patience = 8; wait=0\n",
    "\n",
    "    for ep in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        idx = torch.randperm(Xtr.shape[0])\n",
    "        for i in range(0, Xtr.shape[0], batch):\n",
    "            b = idx[i:i+batch]\n",
    "            xb, yb = Xtr[b], ytr[b]\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            p = model(Xva)\n",
    "            rmse = float(torch.sqrt(loss_fn(p, yva)).item())\n",
    "        if rmse < best_rmse - 1e-4:\n",
    "            best_rmse = rmse\n",
    "            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    torch_path = os.path.join(scenario_dir, \"model_TorchMLP.pt\")\n",
    "    torch.save(model.state_dict(), torch_path)\n",
    "    return model.to('cpu'), {\"best_valid_RMSE\": best_rmse, \"saved_path\": torch_path}\n",
    "\n",
    "# =====================\n",
    "# 7) 시나리오별 실행 루프\n",
    "# =====================\n",
    "def run_scenario(scn: Dict):\n",
    "    name = scn[\"name\"]\n",
    "    include_stages = scn.get(\"include\", [])\n",
    "    exclude_stages = scn.get(\"exclude\", [])\n",
    "\n",
    "    scenario_dir = os.path.join(OUT_DIR, \"scenarios\", name)\n",
    "    os.makedirs(scenario_dir, exist_ok=True)\n",
    "\n",
    "    # ----- 피처 선택 -----\n",
    "    cols = list(X_all.columns)\n",
    "    include_cols = cols_of_stages(cols, include_stages)\n",
    "    if exclude_stages:\n",
    "        exclude_cols = cols_of_stages(cols, exclude_stages)\n",
    "        include_cols = [c for c in include_cols if c not in exclude_cols]\n",
    "\n",
    "    Xscenario = X_all[include_cols].copy()\n",
    "    y = pd.to_numeric(y_all, errors=\"coerce\")\n",
    "\n",
    "    # NaN 방어 (y NaN 제거)\n",
    "    mask = ~y.isna()\n",
    "    Xscenario = Xscenario.loc[mask]\n",
    "    y = y.loc[mask]\n",
    "\n",
    "    # 상수 피처 제거\n",
    "    if len(Xscenario.columns) == 0:\n",
    "        print(f\"[WARN] Scenario '{name}' has 0 features after selection.\")\n",
    "        return\n",
    "    desc = Xscenario.describe().T\n",
    "    keep = desc.index[desc[\"std\"].fillna(0) > 0].tolist()\n",
    "    Xscenario = Xscenario[keep]\n",
    "    dropped_in_scenario = [c for c in include_cols if c not in keep]\n",
    "\n",
    "    # 누수 의심 제거\n",
    "    scenario_suspects = [c for c in overall_suspects if c in Xscenario.columns]\n",
    "    if AUTO_DROP_LEAKS and scenario_suspects:\n",
    "        Xscenario = Xscenario.drop(columns=scenario_suspects)\n",
    "\n",
    "    # ----- 분할 -----\n",
    "    if dt_col is not None:\n",
    "        order = df.loc[Xscenario.index, dt_col].sort_values().index\n",
    "        Xs, ys = Xscenario.loc[order], y.loc[order]\n",
    "        n = len(Xs)\n",
    "        n_tr = int(n*0.70); n_va = int(n*0.15)\n",
    "        X_train, y_train = Xs.iloc[:n_tr], ys.iloc[:n_tr]\n",
    "        X_valid, y_valid = Xs.iloc[n_tr:n_tr+n_va], ys.iloc[n_tr:n_tr+n_va]\n",
    "        X_test,  y_test  = Xs.iloc[n_tr+n_va:], ys.iloc[n_tr+n_va:]\n",
    "    else:\n",
    "        X_fill = Xscenario.copy()\n",
    "        for c in X_fill.columns:\n",
    "            if X_fill[c].dtype.kind not in 'biufc':\n",
    "                X_fill[c] = X_fill[c].astype(str)\n",
    "        row_hash = pd.util.hash_pandas_object(X_fill.fillna('NA'), index=False).astype('int64')\n",
    "        gss1 = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=RANDOM_SEED)\n",
    "        idx_trv, idx_te = next(gss1.split(Xscenario, y, groups=row_hash))\n",
    "        X_trv, y_trv, g_trv = Xscenario.iloc[idx_trv], y.iloc[idx_trv], row_hash.iloc[idx_trv]\n",
    "        X_test, y_test = Xscenario.iloc[idx_te], y.iloc[idx_te]\n",
    "\n",
    "        gss2 = GroupShuffleSplit(n_splits=1, test_size=0.1765, random_state=RANDOM_SEED)\n",
    "        idx_tr, idx_va = next(gss2.split(X_trv, y_trv, groups=g_trv))\n",
    "        X_train, y_train = X_trv.iloc[idx_tr], y_trv.iloc[idx_tr]\n",
    "        X_valid, y_valid = X_trv.iloc[idx_va], y_trv.iloc[idx_va]\n",
    "\n",
    "    # ----- 전처리기 -----\n",
    "    preprocess_plain, preprocess_scaled, num_features, cat_features = make_preprocessors(Xscenario)\n",
    "\n",
    "    # ----- 모델들 -----\n",
    "    models = get_models()\n",
    "\n",
    "    results = []\n",
    "    trained_pipes: Dict[str, Pipeline] = {}\n",
    "    preds_valid, preds_test = {}, {}\n",
    "\n",
    "    # ===== 학습/평가 루프 =====\n",
    "    for mname, mdl in models.items():\n",
    "        print(f\"\\n[Scenario: {name}] ▶ Training {mname} ...\")\n",
    "        pipe, prep = build_pipe(mname, mdl, preprocess_plain, preprocess_scaled)\n",
    "\n",
    "        # booster early stopping용 변환 검증셋\n",
    "        Xv_t = None\n",
    "        if mname in {'XGBoost','LightGBM','CatBoost'}:\n",
    "            prep_eval = clone(prep)\n",
    "            prep_eval.fit(X_train, y_train)\n",
    "            Xv_t = prep_eval.transform(X_valid)\n",
    "\n",
    "        fit_kwargs = {}\n",
    "        if mname in {'XGBoost','LightGBM','CatBoost'} and Xv_t is not None:\n",
    "            fit_kwargs['model__eval_set'] = [(Xv_t, y_valid)] if mname != 'CatBoost' else (Xv_t, y_valid)\n",
    "\n",
    "            if mname == 'XGBoost':\n",
    "                if fit_accepts(mdl, 'eval_metric'): fit_kwargs['model__eval_metric'] = 'rmse'\n",
    "                if fit_accepts(mdl, 'early_stopping_rounds'): fit_kwargs['model__early_stopping_rounds'] = 100\n",
    "                elif fit_accepts(mdl, 'callbacks'):\n",
    "                    try:\n",
    "                        import xgboost as xgb\n",
    "                        fit_kwargs['model__callbacks'] = [\n",
    "                            xgb.callback.EarlyStopping(rounds=100, save_best=True),\n",
    "                            xgb.callback.EvaluationMonitor(period=PROGRESS_PERIOD)\n",
    "                        ]\n",
    "                    except Exception: pass\n",
    "                if fit_accepts(mdl, 'verbose'): fit_kwargs['model__verbose'] = True\n",
    "\n",
    "            elif mname == 'LightGBM':\n",
    "                if fit_accepts(mdl, 'early_stopping_rounds'): fit_kwargs['model__early_stopping_rounds'] = 100\n",
    "                if fit_accepts(mdl, 'eval_metric'): fit_kwargs['model__eval_metric'] = 'rmse'\n",
    "                if fit_accepts(mdl, 'callbacks'):\n",
    "                    try:\n",
    "                        import lightgbm as lgb\n",
    "                        fit_kwargs['model__callbacks'] = [\n",
    "                            lgb.early_stopping(100, verbose=False),\n",
    "                            lgb.log_evaluation(PROGRESS_PERIOD)\n",
    "                        ]\n",
    "                    except Exception: pass\n",
    "                if fit_accepts(mdl, 'verbose'): fit_kwargs['model__verbose'] = True\n",
    "\n",
    "        try:\n",
    "            pipe.fit(X_train, y_train, **fit_kwargs)\n",
    "            yv = pipe.predict(X_valid);  yt = pipe.predict(X_test)\n",
    "            v_mae, v_rmse, v_r2 = evaluate(y_valid, yv)\n",
    "            t_mae, t_rmse, t_r2 = evaluate(y_test,  yt)\n",
    "\n",
    "            results.append({'model': mname,'val_MAE': v_mae,'val_RMSE': v_rmse,'val_R2': v_r2,\n",
    "                            'test_MAE': t_mae,'test_RMSE': t_rmse,'test_R2': t_r2})\n",
    "            trained_pipes[mname] = pipe\n",
    "            preds_valid[mname] = yv\n",
    "            preds_test[mname]  = yt\n",
    "\n",
    "            joblib.dump(pipe, os.path.join(scenario_dir, f\"model_{mname}.pkl\"))\n",
    "            print(f\"[OK] {mname}: test_R2={t_r2:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] {mname}: {e}\")\n",
    "            results.append({'model': mname, 'error': str(e)})\n",
    "\n",
    "    # ===== Torch MLP (AI) =====\n",
    "    torch_info = {}\n",
    "    if have_torch:\n",
    "        num_cols_only = Xscenario.select_dtypes(include=[np.number]).columns\n",
    "        if len(num_cols_only) > 0:\n",
    "            Xtr_n, Xva_n, Xte_n, scaler, num_list = prep_numeric_for_torch(X_train, X_valid, X_test, num_cols_only)\n",
    "            torch_model, torch_info = train_torch_mlp(\n",
    "                Xtr_n, y_train.reset_index(drop=True),\n",
    "                Xva_n, y_valid.reset_index(drop=True),\n",
    "                scenario_dir, max_epochs=50, lr=1e-3, batch=2048\n",
    "            )\n",
    "            if torch_model is not None:\n",
    "                import torch\n",
    "                torch_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    yv = torch_model(torch.tensor(Xva_n.astype(np.float32).values)).cpu().numpy()\n",
    "                    yt = torch_model(torch.tensor(Xte_n.astype(np.float32).values)).cpu().numpy()\n",
    "                yv = np.nan_to_num(yv, nan=np.nanmedian(yv))\n",
    "                yt = np.nan_to_num(yt, nan=np.nanmedian(yt))\n",
    "                v_mae, v_rmse, v_r2 = evaluate(y_valid, yv)\n",
    "                t_mae, t_rmse, t_r2 = evaluate(y_test,  yt)\n",
    "                results.append({'model':'TorchMLP','val_MAE':v_mae,'val_RMSE':v_rmse,'val_R2':v_r2,\n",
    "                                'test_MAE':t_mae,'test_RMSE':t_rmse,'test_R2':t_r2,'note': torch_info})\n",
    "                preds_valid['TorchMLP'] = yv\n",
    "                preds_test['TorchMLP']  = yt\n",
    "                joblib.dump({'scaler': scaler, 'num_cols': num_list}, os.path.join(scenario_dir, \"torch_preproc.pkl\"))\n",
    "        else:\n",
    "            print(\"[Torch] No numeric columns in scenario; skipping TorchMLP.\")\n",
    "\n",
    "    # ===== 메트릭/리더보드 저장 =====\n",
    "    metrics = pd.DataFrame(results)\n",
    "    metric_cols = ['model','val_MAE','val_RMSE','val_R2','test_MAE','test_RMSE','test_R2','error','note']\n",
    "    metrics = metrics.reindex(columns=[c for c in metric_cols if c in metrics.columns])\n",
    "    if 'test_R2' in metrics.columns:\n",
    "        metrics = metrics.sort_values(by='test_R2', ascending=False, na_position='last')\n",
    "    for c in ['val_MAE','val_RMSE','val_R2','test_MAE','test_RMSE','test_R2']:\n",
    "        if c in metrics.columns:\n",
    "            metrics[c] = pd.to_numeric(metrics[c], errors='coerce').round(4)\n",
    "    metrics_path = os.path.join(scenario_dir, 'metrics.csv')\n",
    "    metrics.to_csv(metrics_path, index=False)\n",
    "\n",
    "    # 리더보드 PNG\n",
    "    if 'test_R2' in metrics.columns and not metrics.dropna(subset=['test_R2']).empty:\n",
    "        rank = metrics.dropna(subset=['test_R2']).copy().sort_values('test_R2', ascending=True)\n",
    "        plt.figure(figsize=(7, max(3, 0.6*len(rank))))\n",
    "        plt.barh(rank['model'], rank['test_R2'])\n",
    "        plt.xlabel('Test R²'); plt.title(f'Leaderboard — {name}')\n",
    "        save_fig(os.path.join(scenario_dir, 'leaderboard_testR2.png'))\n",
    "\n",
    "    # ===== 베스트 모델 산출물 (예측/플롯) =====\n",
    "    if 'test_R2' in metrics.columns and not metrics.dropna(subset=['test_R2']).empty:\n",
    "        best_name = metrics.iloc[0]['model']\n",
    "        y_pred_test = preds_test[best_name]\n",
    "        out_df = pd.DataFrame({'y_true': y_test.values, 'y_pred': y_pred_test}, index=X_test.index)\n",
    "        if dt_col is not None:\n",
    "            out_df[dt_col] = df.loc[X_test.index, dt_col]\n",
    "        out_df.to_csv(os.path.join(scenario_dir, f\"best_predictions__{best_name}.csv\"), index=False)\n",
    "\n",
    "        # 산점도\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.scatter(y_test, y_pred_test, alpha=0.6)\n",
    "        plt.xlabel(\"y_true (test)\"); plt.ylabel(\"y_pred (test)\")\n",
    "        plt.title(f\"Scatter | {best_name} | {name} | R2={r2_score(y_test, y_pred_test):.3f}\")\n",
    "        save_fig(os.path.join(scenario_dir, f\"scatter_{best_name}.png\"))\n",
    "\n",
    "        # 잔차\n",
    "        resid = y_test.values - y_pred_test\n",
    "        plt.figure(figsize=(8,3))\n",
    "        plt.plot(resid)\n",
    "        plt.xlabel(\"test index\"); plt.ylabel(\"residual\")\n",
    "        plt.title(f\"Residuals | {best_name} | {name}\")\n",
    "        save_fig(os.path.join(scenario_dir, f\"residuals_{best_name}.png\"))\n",
    "\n",
    "        # 시간 플롯(있을 때만)\n",
    "        if dt_col is not None:\n",
    "            tvals = df.loc[X_test.index, dt_col]\n",
    "            order = np.argsort(pd.to_datetime(tvals).values.astype('datetime64[ns]'))\n",
    "            plt.figure(figsize=(8,3))\n",
    "            plt.plot(tvals.values[order], y_test.values[order], label='Actual')\n",
    "            plt.plot(tvals.values[order], y_pred_test[order], label='Pred')\n",
    "            plt.legend(); plt.xlabel(str(dt_col)); plt.ylabel(TARGET_COL)\n",
    "            plt.title(f\"Actual vs Pred over time | {best_name} | {name}\")\n",
    "            save_fig(os.path.join(scenario_dir, f\"timeplot_{best_name}.png\"))\n",
    "\n",
    "    # ===== 각 모델별 Permutation Importance 저장 =====\n",
    "    def get_feature_names(prep: ColumnTransformer):\n",
    "        names = []\n",
    "        if 'num' in prep.named_transformers_:\n",
    "            try:\n",
    "                names += list(prep.named_transformers_['num'].feature_names_in_)\n",
    "            except Exception:\n",
    "                # fallback\n",
    "                for tname, trans, cols in prep.transformers_:\n",
    "                    if tname == 'num': names += list(cols)\n",
    "        if 'cat' in prep.named_transformers_:\n",
    "            cat_cols = []\n",
    "            for tname, trans, cols in prep.transformers_:\n",
    "                if tname == 'cat': cat_cols = list(cols)\n",
    "            ohe = prep.named_transformers_['cat'].named_steps.get('ohe')\n",
    "            try:\n",
    "                names += list(ohe.get_feature_names_out(cat_cols))\n",
    "            except Exception:\n",
    "                names += cat_cols\n",
    "        return names\n",
    "        \n",
    "\n",
    "    # Permutation Importance 실행 여부 설정\n",
    "    ENABLE_PERM_IMPORTANCE = False   # True로 바꾸면 다시 실행됨\n",
    "    \n",
    "    for mname, pipe in list(trained_pipes.items()):\n",
    "        try:\n",
    "            if ENABLE_PERM_IMPORTANCE:\n",
    "                perm = permutation_importance(pipe, X_valid, y_valid,\n",
    "                                              n_repeats=1,   # 반복 최소화\n",
    "                                              n_jobs=1)      # 병렬 대신 직렬\n",
    "                perm_means = pd.Series(perm.importances_mean,\n",
    "                                       index=feats if not isinstance(pipe[-1], TorchMLP) else X_train.columns)\n",
    "                print(f\"[INFO] {mname} PermImp: top5 -> {perm_means.sort_values(ascending=False).head().to_dict()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] PermImp({mname}): {e}\")\n",
    "    \n",
    "\n",
    "    # ===== 사용/제외 컬럼, 설정 저장 =====\n",
    "    used_cols = list(Xscenario.columns)\n",
    "    excluded_cols = [c for c in X_all.columns if c not in used_cols]\n",
    "    pd.DataFrame({'feature': used_cols}).to_csv(os.path.join(scenario_dir, 'features_used.csv'), index=False)\n",
    "    pd.DataFrame({'feature': excluded_cols}).to_csv(os.path.join(scenario_dir, 'features_excluded.csv'), index=False)\n",
    "\n",
    "    config = {\n",
    "        \"scenario\": name,\n",
    "        \"include_stages\": include_stages,\n",
    "        \"exclude_stages\": exclude_stages,\n",
    "        \"dt_col\": dt_col,\n",
    "        \"target\": TARGET_COL,\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"rows_total\": int(len(X_all)),\n",
    "        \"rows_used\": int(len(Xscenario)),\n",
    "        \"suspect_leakage_dropped\": scenario_suspects if AUTO_DROP_LEAKS else [],\n",
    "        \"dropped_prefilter\": dropped_cols,\n",
    "        \"dropped_in_scenario_constant\": dropped_in_scenario\n",
    "    }\n",
    "    with open(os.path.join(scenario_dir, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[DONE] Scenario '{name}' — outputs at: {scenario_dir}\")\n",
    "\n",
    "# =====================\n",
    "# 8) MAIN LOOP — run all scenarios\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    for scn in SCENARIOS:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Running scenario: {scn['name']}\")\n",
    "        print(\"=\"*60)\n",
    "        run_scenario(scn)\n",
    "    print(\"\\nAll scenarios done. Check 'outputs/scenarios/<name>/' folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05d2c7-2ab6-4704-937e-0eaa80eb2d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d1c23-4115-4d84-9c1c-02bb409f62a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2bc056-1ede-4869-829c-53686011a6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be10cb2-1b1c-4bce-b27b-988273b80402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
