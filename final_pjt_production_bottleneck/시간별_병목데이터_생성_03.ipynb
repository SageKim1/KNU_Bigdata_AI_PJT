{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2e8a4-55a3-474b-8a5e-5787e8025a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f168df-ec21-4fe1-a005-58244dce0190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[디버그] 2025-03-15 시간대 최댓값 큐 상위 10개:\n",
      "Warehouse1_Queue           25\n",
      "Blanking_SKU1_Queue         9\n",
      "Forklift_Blanking_Queue     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[디버그] 2025-06-30 시간대 최댓값 큐 상위 10개:\n",
      "Warehouse1_Queue           24\n",
      "Blanking_SKU1_Queue         9\n",
      "Forklift_Blanking_Queue     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[OK] Saved -> ./FinalResults_time_scheduled_20240101_20250630.csv\n",
      "입력 날짜 수: 547 | 실제 행수 = 19,145\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "하루 단위 → 시간 단위(시/스케줄 블록) 공정 시뮬레이션 (Press 과대 방지 캘리브레이션)\n",
    "- 입력 CSV: subset_20240101_20250630_fullcols.csv\n",
    "- 출력 CSV: FinalResults_time_scheduled_20240101_20250630.csv\n",
    "- Little's Law 기반 분해 후,\n",
    "  (1) 우선순위 가중치 → (2) 원본 퍼센타일 상한 캡 → (3) 일평균 보정(재정규화)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# 1) CONFIG\n",
    "# =========================\n",
    "INPUT_CSV   = \"./subset_20240101_20250630_fullcols.csv\"\n",
    "OUTPUT_CSV  = \"./FinalResults_time_scheduled_20240101_20250630.csv\"\n",
    "GRANULARITY = \"hourly\"  # \"hourly\" or \"block\"\n",
    "\n",
    "# 스케줄 (07:50 ~ 다음날 07:40, 20:10~20:40 휴식 포함)\n",
    "SCHEDULE_BLOCKS = [\n",
    "    (\"07:50\",\"09:50\",\"주간\"),\n",
    "    (\"09:50\",\"10:00\",\"휴식\"),\n",
    "    (\"10:00\",\"11:50\",\"주간\"),\n",
    "    (\"11:50\",\"12:40\",\"중식\"),\n",
    "    (\"12:40\",\"14:40\",\"주간\"),\n",
    "    (\"14:40\",\"14:50\",\"휴식\"),\n",
    "    (\"14:50\",\"16:40\",\"주간\"),\n",
    "    (\"16:40\",\"17:10\",\"석식\"),\n",
    "    (\"17:10\",\"20:10\",\"잔업\"),\n",
    "    (\"20:10\",\"20:40\",\"휴식\"),\n",
    "    (\"20:40\",\"00:00\",\"야간\"),\n",
    "    (\"00:00\",\"00:30\",\"야식\"),\n",
    "    (\"00:30\",\"04:00\",\"야간\"),\n",
    "    (\"04:00\",\"05:00\",\"휴식\"),\n",
    "    (\"05:00\",\"07:40\",\"야간\"),\n",
    "]\n",
    "\n",
    "# 근무/휴식 가중치 (생산 분배 및 Util/Time 변조)\n",
    "SHIFT_WEIGHTS = {\"주간\":1.0, \"잔업\":0.9, \"야간\":1.2, \"중식\":0.0, \"석식\":0.0, \"야식\":0.0, \"휴식\":0.0}\n",
    "UTIL_BAND = 0.15     # ±15%\n",
    "TIME_BAND = 0.10     # ±10%\n",
    "\n",
    "# === 병목 순위 반영용 우선순위 가중치(Press 축소) ===\n",
    "#   - 상위 두 개는 살짝 ↑, 중하위는 1.0 근처, Press류는 <1로 ↓\n",
    "CALIBRATION_WEIGHTS = {\n",
    "    \"Warehouse1_Queue\":        1.20,\n",
    "    \"Forklift_Blanking_Queue\": 1.10,\n",
    "    \"Warehouse_3_Queue\":       1.05,\n",
    "    \"Warehouse_4_Queue\":       1.02,\n",
    "    \"Forklift_Press_Queue\":    0.95,\n",
    "    \"Press4_Queue\":            0.90,\n",
    "    \"Press2_Queue\":            0.88,\n",
    "    \"Blanking_SKU3_Queue\":     0.98,\n",
    "    # 지정 외 나머지 기본값:\n",
    "    \"_default_\":               0.95\n",
    "}\n",
    "\n",
    "# === 원본 기반 상한 캡(퍼센타일) ===\n",
    "PCTL_FOR_CAP = 95  # 90/95/99 등 조정 가능\n",
    "CAP_MULTIPLIER = 1.00  # pctl×1.0이 상한. 약간 여유 주려면 1.1 등\n",
    "\n",
    "# =========================\n",
    "# 2) HELPERS\n",
    "# =========================\n",
    "def to_dt(s):\n",
    "    try: return pd.to_datetime(s, errors=\"coerce\")\n",
    "    except: return pd.to_datetime(s.astype(str), errors=\"coerce\")\n",
    "\n",
    "def detect_date_col(df: pd.DataFrame) -> str:\n",
    "    for c in [\"Time_Now\", \"Timestamp\", \"timestamp\", \"Date\", \"date\"]:\n",
    "        if c in df.columns: return c\n",
    "    return df.columns[0]\n",
    "\n",
    "def day_floor(dt: pd.Series) -> pd.Series:\n",
    "    return dt.dt.floor(\"D\")\n",
    "\n",
    "def aggregate_to_daily(df: pd.DataFrame, date_col: str):\n",
    "    \"\"\"일 단위 집계(생산 sum / 시간·유틸·큐 mean / 나머지 first)\"\"\"\n",
    "    dt = to_dt(df[date_col])\n",
    "    if dt.isna().all():\n",
    "        raise ValueError(f\"Could not parse datetime from '{date_col}'\")\n",
    "    df = df.copy()\n",
    "    df[\"_date\"] = day_floor(dt)\n",
    "\n",
    "    count_cols = [c for c in df.columns if c.startswith(\"c_Cell\") or c == \"c_TotalProducts\"]\n",
    "    time_cols  = [c for c in df.columns if c.endswith(\"_Time\")]\n",
    "    util_cols  = [c for c in df.columns if c.endswith(\"_Util\")]\n",
    "    queue_cols = [c for c in df.columns if c.endswith(\"_Queue\")]\n",
    "    other_cols = sorted(set(df.columns) - set([date_col, \"_date\"] + count_cols + time_cols + util_cols + queue_cols))\n",
    "\n",
    "    agg = {}\n",
    "    for c in count_cols: agg[c] = \"sum\"\n",
    "    for c in time_cols + util_cols + queue_cols: agg[c] = \"mean\"\n",
    "    for c in other_cols: agg[c] = \"first\"\n",
    "\n",
    "    daily = df.groupby(\"_date\", as_index=False).agg(agg).sort_values(\"_date\").reset_index(drop=True)\n",
    "    daily[date_col] = daily[\"_date\"]\n",
    "    return daily.drop(columns=[\"_date\"]), count_cols, time_cols, util_cols, queue_cols, date_col\n",
    "\n",
    "def build_schedule_for_day(day: pd.Timestamp) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    anchor_start = pd.Timestamp(day.year, day.month, day.day, 7, 50)\n",
    "    anchor_end   = pd.Timestamp(day.year, day.month, day.day, 7, 40) + pd.Timedelta(days=1)\n",
    "\n",
    "    for start_str, end_str, label in SCHEDULE_BLOCKS:\n",
    "        s_h, s_m = map(int, start_str.split(\":\"))\n",
    "        e_h, e_m = map(int, end_str.split(\":\"))\n",
    "        s = pd.Timestamp(day.year, day.month, day.day, s_h, s_m)\n",
    "        e = pd.Timestamp(day.year, day.month, day.day, e_h, e_m)\n",
    "        if (s_h, s_m) < (7,50): s += pd.Timedelta(days=1)\n",
    "        if (e_h, e_m) < (7,50) or e <= s: e += pd.Timedelta(days=1)\n",
    "        s = max(s, anchor_start); e = min(e, anchor_end)\n",
    "        if e <= s: \n",
    "            continue\n",
    "        minutes = int((e - s).total_seconds() // 60)\n",
    "        rows.append({\"start\": s, \"end\": e, \"label\": label, \"minutes\": minutes})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def explode_to_hours(sch: pd.DataFrame) -> pd.DataFrame:\n",
    "    t_min = sch[\"start\"].min().floor(\"h\")\n",
    "    t_max = sch[\"end\"].max().ceil(\"h\")\n",
    "    hours = pd.date_range(t_min, t_max, freq=\"h\")\n",
    "    slots = pd.DataFrame({\"slot_start\": hours[:-1], \"slot_end\": hours[1:]})\n",
    "    out = []\n",
    "    for _, blk in sch.iterrows():\n",
    "        for _, sl in slots.iterrows():\n",
    "            s = max(blk[\"start\"], sl[\"slot_start\"])\n",
    "            e = min(blk[\"end\"], sl[\"slot_end\"])\n",
    "            overlap = (e - s).total_seconds() / 60.0\n",
    "            if overlap > 0:\n",
    "                out.append({\n",
    "                    \"slot_start\": sl[\"slot_start\"],\n",
    "                    \"slot_end\": sl[\"slot_end\"],\n",
    "                    \"label\": blk[\"label\"],\n",
    "                    \"min_overlap\": overlap\n",
    "                })\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def integer_allocate(total: float, weights: np.ndarray) -> np.ndarray:\n",
    "    w = np.clip(np.asarray(weights, float), 0, None)\n",
    "    if w.sum() <= 0 or total <= 0: return np.zeros_like(w, dtype=int)\n",
    "    p = w / w.sum()\n",
    "    alloc = np.floor(total * p).astype(int)\n",
    "    residual = int(round(total - alloc.sum()))\n",
    "    if residual > 0:\n",
    "        idx = np.argsort(-p)[:residual]\n",
    "        alloc[idx] += 1\n",
    "    return alloc\n",
    "\n",
    "# =========================\n",
    "# 3) MAIN TRANSFORM\n",
    "# =========================\n",
    "def make_time_scheduled(df_daily: pd.DataFrame,\n",
    "                        count_cols, time_cols, util_cols, queue_cols,\n",
    "                        date_col: str,\n",
    "                        granularity: str,\n",
    "                        global_caps: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    # SKU별 Wait 컬럼 매핑 (예: \"SKU3_Wait_Time\")\n",
    "    sku_wait_cols = {c.split(\"_\")[0]: c for c in time_cols if c.startswith(\"SKU\") and c.endswith(\"Wait_Time\")}\n",
    "    overall_wait_name = next((c for c in time_cols if c.endswith(\"Wait_Time\")), None)\n",
    "\n",
    "    for _, day_row in df_daily.iterrows():\n",
    "        day = pd.to_datetime(day_row[date_col]).floor(\"D\")\n",
    "        sch = build_schedule_for_day(day)\n",
    "        if sch.empty:\n",
    "            continue\n",
    "\n",
    "        if granularity == \"block\":\n",
    "            slot_df = sch.rename(columns={\"start\":\"slot_start\",\"end\":\"slot_end\"})\n",
    "            slot_df[\"min_overlap\"] = (slot_df[\"slot_end\"] - slot_df[\"slot_start\"]).dt.total_seconds()/60.0\n",
    "            slot_df = slot_df[[\"slot_start\",\"slot_end\",\"label\",\"min_overlap\"]]\n",
    "        else:\n",
    "            slot_df = explode_to_hours(sch)\n",
    "\n",
    "        # 생산 가중치\n",
    "        slot_df[\"shift_weight\"] = slot_df[\"label\"].map(SHIFT_WEIGHTS).fillna(0.0)\n",
    "        slot_df[\"work_minutes\"] = slot_df[\"min_overlap\"] * (slot_df[\"shift_weight\"] > 0).astype(float)\n",
    "        slot_df[\"prod_weight\"] = slot_df[\"work_minutes\"] * slot_df[\"shift_weight\"]\n",
    "\n",
    "        # count 분배(정수)\n",
    "        for c in count_cols:\n",
    "            total = float(day_row[c])\n",
    "            slot_df[c] = integer_allocate(total, slot_df[\"prod_weight\"].values)\n",
    "\n",
    "        # Util/Time 분배\n",
    "        safe_eps = 1e-12\n",
    "        work_mask = slot_df[\"shift_weight\"].values > 0\n",
    "        sw = slot_df[\"shift_weight\"].values.copy()\n",
    "        if work_mask.any():\n",
    "            sw_norm = np.zeros_like(sw, dtype=float)\n",
    "            mean_sw = sw[work_mask].mean()\n",
    "            sw_norm[work_mask] = sw[work_mask] / (mean_sw + safe_eps)\n",
    "        else:\n",
    "            sw_norm = np.zeros_like(sw, dtype=float)\n",
    "\n",
    "        for c in util_cols:\n",
    "            mean_u = float(day_row[c])\n",
    "            u = np.zeros(len(slot_df), dtype=float)\n",
    "            u[work_mask] = mean_u * (1.0 - UTIL_BAND + 2*UTIL_BAND * sw_norm[work_mask])\n",
    "            slot_df[c] = np.clip(u, 0, 1)\n",
    "\n",
    "        for c in time_cols:\n",
    "            mean_t = float(day_row[c])\n",
    "            t = np.zeros(len(slot_df), dtype=float)\n",
    "            if mean_t != 0 and work_mask.any():\n",
    "                t[work_mask] = mean_t * (1.0 - TIME_BAND + 2*TIME_BAND * sw_norm[work_mask])\n",
    "            slot_df[c] = t\n",
    "\n",
    "        # Queue(L = λ×W) — 1차 계산\n",
    "        slot_df[\"slot_seconds\"] = (slot_df[\"slot_end\"] - slot_df[\"slot_start\"]).dt.total_seconds()\n",
    "        per_slot_total = slot_df[count_cols].sum(axis=1).astype(float)\n",
    "        lam = (per_slot_total / slot_df[\"slot_seconds\"].replace(0, np.nan)).fillna(0.0)\n",
    "\n",
    "        for qc in queue_cols:\n",
    "            daily_mean_q = float(day_row[qc])\n",
    "\n",
    "            # SKUx 매칭\n",
    "            sku_tag = next((p for p in qc.split(\"_\") if p.startswith(\"SKU\")), None)\n",
    "            if sku_tag and sku_tag in sku_wait_cols:\n",
    "                inferred_W = slot_df[sku_wait_cols[sku_tag]].astype(float)\n",
    "            elif overall_wait_name is not None:\n",
    "                inferred_W = slot_df[overall_wait_name].astype(float)\n",
    "            else:\n",
    "                inferred_W = pd.Series(np.zeros(len(slot_df)))\n",
    "\n",
    "            L = lam * inferred_W\n",
    "            if (L > 0).any():\n",
    "                scale = (daily_mean_q / (L.mean() + 1e-9)) if daily_mean_q != 0 else 0.0\n",
    "                q_vals = (L * scale).values\n",
    "            else:\n",
    "                w = slot_df[\"prod_weight\"].values\n",
    "                w = w / (w.sum() + 1e-9)\n",
    "                q_vals = w * daily_mean_q * len(slot_df)\n",
    "\n",
    "            # ---------- (A) 우선순위 가중치 적용 ----------\n",
    "            weight = CALIBRATION_WEIGHTS.get(qc, CALIBRATION_WEIGHTS[\"_default_\"])\n",
    "            q_vals = q_vals * weight\n",
    "\n",
    "            # ---------- (B) 원본 기반 상한 캡 ----------\n",
    "            cap = global_caps.get(qc, None)\n",
    "            if cap is not None:\n",
    "                q_vals = np.minimum(q_vals, cap)\n",
    "\n",
    "            slot_df[qc] = q_vals\n",
    "\n",
    "        # ---------- (C) 일평균 보정(재정규화) ----------\n",
    "        #   우선순위/캡 적용으로 큐 평균이 변했을 수 있으므로, 각 큐별로\n",
    "        #   (원래 일평균)과 (현재 일평균)을 맞춰주기 위한 스케일을 한 번 더 적용\n",
    "        for qc in queue_cols:\n",
    "            if qc not in slot_df.columns:\n",
    "                continue\n",
    "            orig_mean = float(day_row[qc])\n",
    "            cur_mean = float(slot_df[qc].mean()) if len(slot_df) > 0 else 0.0\n",
    "            if cur_mean > 0 and orig_mean > 0:\n",
    "                slot_df[qc] *= (orig_mean / cur_mean)\n",
    "            elif cur_mean == 0 and orig_mean > 0:\n",
    "                # 모두 0이 돼버렸다면 생산 가중치비로 적분치 보정\n",
    "                w = slot_df[\"prod_weight\"].values\n",
    "                w = w / (w.sum() + 1e-9)\n",
    "                slot_df[qc] = w * orig_mean * len(slot_df)\n",
    "\n",
    "        # 출력 누적\n",
    "        for _, r in slot_df.iterrows():\n",
    "            out = {\"Time_Now\": r[\"slot_start\"], \"Time_End\": r[\"slot_end\"], \"ShiftLabel\": r[\"label\"]}\n",
    "            for c in count_cols + util_cols + time_cols + queue_cols:\n",
    "                out[c] = r.get(c, 0)\n",
    "            rows.append(out)\n",
    "\n",
    "    result = pd.DataFrame(rows).sort_values([\"Time_Now\"]).reset_index(drop=True)\n",
    "\n",
    "    # 컬럼 순서 정리\n",
    "    keep_order, all_cols = [], set(count_cols + util_cols + time_cols + queue_cols)\n",
    "    for c in df_daily.columns:\n",
    "        if c in all_cols: keep_order.append(c)\n",
    "    for c in (set(all_cols) - set(keep_order)): keep_order.append(c)\n",
    "    result = result.reindex(columns=[\"Time_Now\",\"Time_End\",\"ShiftLabel\"] + keep_order)\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) RUN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) 로드\n",
    "    full = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "\n",
    "    # 2) 날짜 컬럼 탐지 및 datetime 변환\n",
    "    date_col = detect_date_col(full)\n",
    "    full[date_col] = to_dt(full[date_col])\n",
    "    if full[date_col].isna().all():\n",
    "        raise ValueError(f\"날짜 파싱 실패: '{date_col}' 컬럼을 datetime으로 변환할 수 없습니다.\")\n",
    "    full[\"_date\"] = full[date_col].dt.floor(\"D\")\n",
    "\n",
    "    # 3) 열 그룹화\n",
    "    count_cols = [c for c in full.columns if c.startswith(\"c_Cell\") or c == \"c_TotalProducts\"]\n",
    "    time_cols  = [c for c in full.columns if c.endswith(\"_Time\")]\n",
    "    util_cols  = [c for c in full.columns if c.endswith(\"_Util\")]\n",
    "    queue_cols = [c for c in full.columns if c.endswith(\"_Queue\")]\n",
    "\n",
    "    # 4) 원본 전체에서 큐별 상한 캡(퍼센타일) 계산\n",
    "    global_caps = {}\n",
    "    for qc in queue_cols:\n",
    "        try:\n",
    "            pctl = np.nanpercentile(full[qc].astype(float).values, PCTL_FOR_CAP)\n",
    "            global_caps[qc] = float(pctl) * CAP_MULTIPLIER\n",
    "        except Exception:\n",
    "            global_caps[qc] = None  # 계산 불가시 캡 미적용\n",
    "\n",
    "    # 5) 입력이 '하루 1행'인지 판단 → 일 집계/스킵\n",
    "    nunique_dates = full[\"_date\"].nunique()\n",
    "    is_daily_like = (len(full) == nunique_dates)\n",
    "\n",
    "    if is_daily_like:\n",
    "        df_daily = full.drop(columns=[\"_date\"]).copy()\n",
    "        df_daily[date_col] = full[\"_date\"].values\n",
    "    else:\n",
    "        df_daily = full.drop(columns=[\"_date\"]).copy()\n",
    "        df_daily, count_cols, time_cols, util_cols, queue_cols, date_col = aggregate_to_daily(df_daily, date_col)\n",
    "\n",
    "    # 6) 시간표 기반 분해 + 캘리브레이션/캡/보정\n",
    "    out = make_time_scheduled(\n",
    "        df_daily,\n",
    "        count_cols=count_cols,\n",
    "        time_cols=time_cols,\n",
    "        util_cols=util_cols,\n",
    "        queue_cols=queue_cols,\n",
    "        date_col=date_col,\n",
    "        granularity=GRANULARITY,\n",
    "        global_caps=global_caps\n",
    "    )\n",
    "\n",
    "    # 7) 저장\n",
    "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 8) 간단 진단: 대표 두 날짜의 \"시간대 최댓값 큐\" 빈도\n",
    "    for tgt in [\"2025-03-15\",\"2025-06-30\"]:\n",
    "        d = pd.Timestamp(tgt)\n",
    "        one = out[out[\"Time_Now\"].dt.floor(\"D\")==d]\n",
    "        if one.empty: \n",
    "            print(f\"[디버그] {tgt} 데이터 없음\"); \n",
    "            continue\n",
    "        # 시간별 최댓값 큐 집계\n",
    "        qmax = one[queue_cols].astype(float).idxmax(axis=1)\n",
    "        print(f\"\\n[디버그] {tgt} 시간대 최댓값 큐 상위 10개:\")\n",
    "        print(qmax.value_counts().head(10))\n",
    "\n",
    "    print(f\"\\n[OK] Saved -> {OUTPUT_CSV}\")\n",
    "    print(f\"입력 날짜 수: {nunique_dates:,} | 실제 행수 = {len(out):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ad786-8582-48c4-977a-f5d7eebf2820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ddabd5-0716-4f14-80a3-58b568d136fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99056e9-55a4-41fc-a06d-0de20860992e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3eada2-dbc6-4cc7-907a-fd3ed9970571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30c672-adee-481a-a0c9-12d67514ea22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c868f09-834f-4982-bd30-96171f141a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379fd9d-e1e3-4293-901b-346d96126454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00771e5-69c5-4523-a9e8-12e72f1658c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21d898-5e53-4cd9-af07-1f6bd0f9a299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ab865-bccd-466b-8b5d-f43be7b30910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453c779-7f9f-4990-a3fe-156536e86a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
