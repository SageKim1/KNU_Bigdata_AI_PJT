{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6633a2-a78a-49ae-ad01-799f530f03c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] RidgeCV: {'MAE': 20.44161865650393, 'RMSE': 24.24855960188918, 'R2': 0.9995147380981386}\n",
      "[REG] RandomForest: {'MAE': 21.26736662269958, 'RMSE': 25.295925496573954, 'R2': 0.9994719130311346}\n",
      "[CLS] Top10% ROC-AUC=1.000\n",
      "\n",
      "[Saved files]\n",
      " - bottleneck_labels.csv\n",
      " - bottleneck_impact_summary.csv (+ bottleneck_impact_with_defects.csv if available)\n",
      " - plot_bottleneck_frequency.png\n",
      " - plot_mean_throughput_by_bottleneck.png\n",
      " - model_RidgeCV_wBN.pkl / predictions_RidgeCV_wBN.csv\n",
      " - model_RF_wBN.pkl / predictions_RF_wBN.csv\n",
      " - regression_metrics.json\n",
      " - feature_importance_RF_with_BN.csv\n",
      " - feature_importance_RF__BN_only.csv (if any)\n",
      " - plot_RF_importance_BN_top20.png (if any)\n",
      " - classification_report_top10pct.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Bottleneck → Throughput 분석 (row-wise max Queue = bottleneck)\n",
    "- 입력: Final Results Extended.csv (하루 단위)\n",
    "- 병목 레이블: 각 행에서 Queue 컬럼 중 최댓값의 컬럼명 (bottleneck_col_top1) + 그 값 (bottleneck_val_top1)\n",
    "- 타깃: c_TotalProducts (없으면 SKU 관련 합산)\n",
    "- 회귀: RidgeCV, RandomForest 로 성능/중요도 비교\n",
    "- 분류(옵션): 생산량 상위 10% vs 나머지 (LogisticRegression)\n",
    "- 산출물: outputs/bottleneck/* 일괄 저장\n",
    "\"\"\"\n",
    "\n",
    "import os, re, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import RidgeCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, roc_auc_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# --------------------------\n",
    "# 0) CONFIG\n",
    "# --------------------------\n",
    "FILE_PATH  = r\"Final Results Extended.csv\"   # ← 경로만 바꿔서 사용\n",
    "TARGET_COL = \"c_TotalProducts\"               # 기본 타깃\n",
    "OUT_DIR    = os.path.join(\"outputs\", \"bottleneck\"); os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Queue 컬럼 인식 규칙(필요시 보강: r\"_Queue$\" 등)\n",
    "QUEUE_PATTERN = re.compile(r\"_Queue\", re.I)\n",
    "\n",
    "# --------------------------\n",
    "# 1) LOAD\n",
    "# --------------------------\n",
    "df = pd.read_csv(FILE_PATH, low_memory=False)\n",
    "\n",
    "# 타깃 보정\n",
    "if TARGET_COL not in df.columns:\n",
    "    sku_cols = [c for c in df.columns if re.search(r\"sku\", c, re.I)]\n",
    "    if not sku_cols:\n",
    "        raise ValueError(\"타깃 c_TotalProducts가 없고 SKU 관련 컬럼도 없어 타깃을 만들 수 없습니다.\")\n",
    "    df[\"__target__\"] = df[sku_cols].sum(axis=1)\n",
    "    TARGET = \"__target__\"\n",
    "else:\n",
    "    TARGET = TARGET_COL\n",
    "\n",
    "y = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "# --------------------------\n",
    "# 2) 병목 레이블링 (row-wise idxmax on Queue columns)\n",
    "# --------------------------\n",
    "queue_cols = [c for c in df.columns if QUEUE_PATTERN.search(c)]\n",
    "if not queue_cols:\n",
    "    raise ValueError(\"Queue 컬럼을 찾지 못했습니다. QUEUE_PATTERN을 수정하세요.\")\n",
    "\n",
    "# 숫자화 & NaN 무한값 방어\n",
    "Q = df[queue_cols].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf,-np.inf], np.nan)\n",
    "# 모두 NaN인 행은 0으로 대체해 idxmax가 동작하도록(대체 룰은 필요시 조정)\n",
    "Q_filled = Q.fillna(-np.inf)\n",
    "\n",
    "bottleneck_idx = Q_filled.idxmax(axis=1)\n",
    "bottleneck_val = Q.max(axis=1, numeric_only=True)\n",
    "\n",
    "df[\"bottleneck_col_top1\"] = bottleneck_idx\n",
    "df[\"bottleneck_val_top1\"] = bottleneck_val\n",
    "\n",
    "# 저장(레이블만)\n",
    "df[[\"bottleneck_col_top1\",\"bottleneck_val_top1\"]].to_csv(\n",
    "    os.path.join(OUT_DIR, \"bottleneck_labels.csv\"), index=False\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 3) 병목별 그룹 통계 (생산량, 결함률 있으면 같이)\n",
    "# --------------------------\n",
    "stats = (\n",
    "    df.groupby(\"bottleneck_col_top1\")[TARGET]\n",
    "      .agg([\"mean\",\"median\",\"std\",\"count\"])\n",
    "      .reset_index()\n",
    "      .sort_values(\"mean\", ascending=False)\n",
    ")\n",
    "overall_mean = float(np.nanmean(y))\n",
    "stats[\"lift_vs_overall\"] = stats[\"mean\"] - overall_mean\n",
    "stats.to_csv(os.path.join(OUT_DIR, \"bottleneck_impact_summary.csv\"), index=False)\n",
    "\n",
    "# (옵션) 불량 관련 컬럼 자동 탐색\n",
    "defect_cols = [c for c in df.columns if re.search(r\"(defect|scrap|ng|fail|reject)\", c, re.I)]\n",
    "if defect_cols:\n",
    "    # 수치형만 평균/중앙/건수\n",
    "    dsum = df.groupby(\"bottleneck_col_top1\")[defect_cols].mean(numeric_only=True)\n",
    "    dsum.columns = [f\"DEFECT_MEAN__{c}\" for c in dsum.columns]\n",
    "    stats2 = df.groupby(\"bottleneck_col_top1\")[defect_cols].median(numeric_only=True)\n",
    "    stats2.columns = [f\"DEFECT_MEDIAN__{c}\" for c in stats2.columns]\n",
    "    defect_summary = pd.concat([stats.set_index(\"bottleneck_col_top1\"), dsum, stats2], axis=1).reset_index()\n",
    "    defect_summary.to_csv(os.path.join(OUT_DIR, \"bottleneck_impact_with_defects.csv\"), index=False)\n",
    "\n",
    "# 시각화: 병목 빈도 & 평균 생산량\n",
    "plt.figure(figsize=(10,4))\n",
    "stats_freq = df[\"bottleneck_col_top1\"].value_counts().sort_values(ascending=False)\n",
    "stats_freq.plot(kind=\"bar\")\n",
    "plt.title(\"Bottleneck Frequency by Stage\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"plot_bottleneck_frequency.png\"), dpi=150); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(stats[\"bottleneck_col_top1\"], stats[\"mean\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Mean Throughput\")\n",
    "plt.title(\"Mean Throughput by Bottleneck\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"plot_mean_throughput_by_bottleneck.png\"), dpi=150); plt.close()\n",
    "\n",
    "# --------------------------\n",
    "# 4) 피처에 병목 포함 → 회귀 성능 확인\n",
    "# --------------------------\n",
    "# X 기본: 원본에서 타깃 제외\n",
    "X_base = df.drop(columns=[TARGET])\n",
    "\n",
    "# 병목 레이블 One-Hot\n",
    "bn_label = df[\"bottleneck_col_top1\"].fillna(\"None\").astype(str).to_frame()\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "bn_ohe = ohe.fit_transform(bn_label)\n",
    "bn_ohe_cols = [f\"BN_{c}\" for c in ohe.categories_[0]]\n",
    "\n",
    "X = pd.concat(\n",
    "    [X_base.reset_index(drop=True).drop(columns=[\"bottleneck_col_top1\"], errors=\"ignore\"),\n",
    "     pd.DataFrame(bn_ohe, columns=bn_ohe_cols)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 타깃 NaN 제거\n",
    "mask = ~y.isna()\n",
    "X, y2 = X.loc[mask].reset_index(drop=True), y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# 간단 전처리: 수치/범주 자동 파이프\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "num_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                   (\"scaler\", StandardScaler())])\n",
    "cat_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                   (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "\n",
    "transformers = []\n",
    "if num_cols: transformers.append((\"num\", num_tf, num_cols))\n",
    "if cat_cols: transformers.append((\"cat\", cat_tf, cat_cols))\n",
    "preprocess = ColumnTransformer(transformers, remainder=\"drop\")\n",
    "\n",
    "# 모델 1: RidgeCV\n",
    "ridge = Pipeline([(\"prep\", preprocess),\n",
    "                  (\"model\", RidgeCV(alphas=np.logspace(-3,3,20)))])\n",
    "# 모델 2: RandomForest\n",
    "rf = Pipeline([(\"prep\", preprocess),\n",
    "               (\"model\", RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1))])\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "def eval_reg(pipe, name):\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    pred = pipe.predict(X_te)\n",
    "    out = {\n",
    "        \"MAE\": float(mean_absolute_error(y_te, pred)),\n",
    "        \"RMSE\": float(np.sqrt(mean_squared_error(y_te, pred))),\n",
    "        \"R2\": float(r2_score(y_te, pred))\n",
    "    }\n",
    "    # 저장\n",
    "    joblib.dump(pipe, os.path.join(OUT_DIR, f\"model_{name}.pkl\"))\n",
    "    pd.DataFrame({\"y_true\": y_te, \"y_pred\": pred}).to_csv(\n",
    "        os.path.join(OUT_DIR, f\"predictions_{name}.csv\"), index=False\n",
    "    )\n",
    "    return out\n",
    "\n",
    "ridge_m = eval_reg(ridge, \"RidgeCV_wBN\")\n",
    "rf_m    = eval_reg(rf,    \"RF_wBN\")\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"regression_metrics.json\"), \"w\") as f:\n",
    "    json.dump({\"RidgeCV\": ridge_m, \"RandomForest\": rf_m}, f, indent=2)\n",
    "\n",
    "print(\"[REG] RidgeCV:\", ridge_m)\n",
    "print(\"[REG] RandomForest:\", rf_m)\n",
    "\n",
    "# RF 피처 중요도(전처리 후 feature_names 추출)\n",
    "def get_feature_names(prep: ColumnTransformer) -> list:\n",
    "    names = []\n",
    "    # num\n",
    "    if \"num\" in prep.named_transformers_:\n",
    "        try:\n",
    "            names += list(prep.named_transformers_[\"num\"].named_steps[\"imp\"].get_feature_names_out(num_cols))\n",
    "        except Exception:\n",
    "            names += num_cols\n",
    "    # cat\n",
    "    if \"cat\" in prep.named_transformers_:\n",
    "        cat_ohe = prep.named_transformers_[\"cat\"].named_steps.get(\"ohe\")\n",
    "        cat_raw = prep.transformers_[1][2] if len(prep.transformers_)>1 else []\n",
    "        try:\n",
    "            names += list(cat_ohe.get_feature_names_out(cat_raw))\n",
    "        except Exception:\n",
    "            names += cat_raw\n",
    "    return names\n",
    "\n",
    "# 다시 학습해서 중요도 뽑기(동일 파이프라인)\n",
    "rf.fit(X_tr, y_tr)\n",
    "rf_feat_names = get_feature_names(rf.named_steps[\"prep\"])\n",
    "if hasattr(rf.named_steps[\"model\"], \"feature_importances_\"):\n",
    "    importances = rf.named_steps[\"model\"].feature_importances_\n",
    "    imp_df = pd.DataFrame({\"feature\": rf_feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "    imp_df.to_csv(os.path.join(OUT_DIR, \"feature_importance_RF_with_BN.csv\"), index=False)\n",
    "\n",
    "    # 병목 원핫만 보기\n",
    "    imp_bn = imp_df[imp_df[\"feature\"].str.startswith(\"cat__ohe__bottleneck_col_top1_\") | imp_df[\"feature\"].str.startswith(\"BN_\")]\n",
    "    if not imp_bn.empty:\n",
    "        imp_bn.to_csv(os.path.join(OUT_DIR, \"feature_importance_RF__BN_only.csv\"), index=False)\n",
    "\n",
    "        plt.figure(figsize=(8, max(3, 0.4*len(imp_bn.head(20)))))\n",
    "        top = imp_bn.head(20)\n",
    "        plt.barh(top[\"feature\"][::-1], top[\"importance\"][::-1])\n",
    "        plt.title(\"RF Importance — Bottleneck One-Hot (Top20)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"plot_RF_importance_BN_top20.png\"), dpi=150); plt.close()\n",
    "\n",
    "# --------------------------\n",
    "# 5) (옵션) 분류: 상위 10% 생산량 vs 나머지\n",
    "# --------------------------\n",
    "q90 = float(np.nanpercentile(y2, 90))\n",
    "cls_y = (y2 >= q90).astype(int)\n",
    "\n",
    "logreg = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, class_weight=\"balanced\", n_jobs=None))\n",
    "])\n",
    "logreg.fit(X_tr, (y_tr >= q90).astype(int))\n",
    "proba = logreg.predict_proba(X_te)[:,1]\n",
    "auc = roc_auc_score((y_te >= q90).astype(int), proba)\n",
    "report = classification_report((y_te >= q90).astype(int), (proba>=0.5).astype(int), output_dict=True)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"classification_report_top10pct.json\"), \"w\") as f:\n",
    "    json.dump({\"ROC_AUC\": auc, \"report\": report}, f, indent=2)\n",
    "\n",
    "print(f\"[CLS] Top10% ROC-AUC={auc:.3f}\")\n",
    "\n",
    "# --------------------------\n",
    "# 저장물 안내\n",
    "# --------------------------\n",
    "print(\"\\n[Saved files]\")\n",
    "print(\" - bottleneck_labels.csv\")\n",
    "print(\" - bottleneck_impact_summary.csv\", \"(+ bottleneck_impact_with_defects.csv if available)\")\n",
    "print(\" - plot_bottleneck_frequency.png\")\n",
    "print(\" - plot_mean_throughput_by_bottleneck.png\")\n",
    "print(\" - model_RidgeCV_wBN.pkl / predictions_RidgeCV_wBN.csv\")\n",
    "print(\" - model_RF_wBN.pkl / predictions_RF_wBN.csv\")\n",
    "print(\" - regression_metrics.json\")\n",
    "print(\" - feature_importance_RF_with_BN.csv\")\n",
    "print(\" - feature_importance_RF__BN_only.csv (if any)\")\n",
    "print(\" - plot_RF_importance_BN_top20.png (if any)\")\n",
    "print(\" - classification_report_top10pct.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73169233-ec89-40f2-b11e-ebf9e8eee1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b17f1-688d-4163-ac50-492ecba71a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760984af-a091-47bc-bf2c-78aacac2d90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e312d2-8e27-4c3b-b1f8-586b888116bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3ea0b0-4c67-4bd8-853e-96c0d37b5bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] A: BN only => {'RidgeCV': {'MAE': 880.413304862572, 'RMSE': 1091.3276455144644, 'R2': 0.01708804409174336}, 'RandomForest': {'MAE': 880.4143148294148, 'RMSE': 1091.3277300986306, 'R2': 0.017087891729083537}}\n",
      "[REG] B: BN + LeadTime => {'RidgeCV': {'MAE': 880.413304862572, 'RMSE': 1091.3276455144644, 'R2': 0.01708804409174336}, 'RandomForest': {'MAE': 880.4143148294147, 'RMSE': 1091.3277300986306, 'R2': 0.017087891729083537}}\n",
      "[CLS] BN+LT Top10% ROC-AUC=0.553\n",
      "\n",
      "[Saved files @ outputs\\bottleneck]\n",
      " - bottleneck_labels.csv\n",
      " - bottleneck_impact_summary.csv (+ bottleneck_impact_with_defects.csv if available)\n",
      " - plot_bottleneck_frequency.png\n",
      " - plot_mean_throughput_by_bottleneck.png\n",
      " - regression_metrics_BNonly.json (Ridge/RF)\n",
      " - regression_metrics_BNplusLT.json (Ridge/RF)\n",
      " - model_RidgeCV_BNonly.pkl / predictions_RidgeCV_BNonly.csv\n",
      " - model_RandomForest_BNonly.pkl / predictions_RandomForest_BNonly.csv\n",
      " - model_RidgeCV_BNplusLT.pkl / predictions_RidgeCV_BNplusLT.csv\n",
      " - model_RandomForest_BNplusLT.pkl / predictions_RandomForest_BNplusLT.csv\n",
      " - feature_importance_RF_BNplusLT.csv\n",
      " - plot_feature_importance_RF_BNplusLT_top20.png\n",
      " - ridge_coeff_BNplusLT.csv\n",
      " - feature_importance_RF__LeadTime_only.csv (if any)\n",
      " - leadtime_importance_pivot_RF.csv / plot_leadtime_importance_RF_heatmap.png (if any)\n",
      " - leadtime_vs_throughput_corr.csv (if any)\n",
      " - classification_report_top10pct_BNplusLT.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Bottleneck → Throughput + Lead-time 영향 분석 (스테이지 대표 Queue 기반)\n",
    "- 입력: Final Results Extended.csv\n",
    "- 병목 레이블: 행 단위로 스테이지별 대표 Queue(예: max(Blanking SKU1~4, Blanking_Queue)) 중 최댓값의 '스테이지'를 bottleneck으로 지정\n",
    "- 타깃: c_TotalProducts (없으면 SKU 합산)\n",
    "- 리드타임: SKU1~4 × {VA,NVA,Transport,Wait,Other}_Time (유연한 정규식으로 탐색)\n",
    "- 모델: RidgeCV, RandomForest (성능/중요도 비교)\n",
    "- 산출물: outputs/bottleneck/* 에 CSV/PNG/PKL 저장\n",
    "\"\"\"\n",
    "\n",
    "import os, re, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import RidgeCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, roc_auc_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# --------------------------\n",
    "# 0) CONFIG\n",
    "# --------------------------\n",
    "FILE_PATH  = r\"Final Results Extended.csv\"   # ← 경로 조정\n",
    "TARGET_COL = \"c_TotalProducts\"\n",
    "OUT_DIR    = os.path.join(\"outputs\", \"bottleneck\"); os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 병목 선택 기준: 'raw' = 원시 Queue값으로 비교, 'zscore' = 스테이지별 z-score로 비교(스케일 보정)\n",
    "BN_SELECTION = \"raw\"   # 'raw' or 'zscore'\n",
    "# Blanking 대표값 산출 전략: 'prefer_sku' = SKU1~4만 사용(요약컬럼이 있어도 무시), 'max_all' = SKU들과 요약컬럼 모두의 최대\n",
    "BLANKING_STRATEGY = \"prefer_sku\"  # 'prefer_sku' or 'max_all'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.20\n",
    "TOP_K_IMP = 20  # 중요도 그래프 top-K\n",
    "\n",
    "# --------------------------\n",
    "# 1) LOAD\n",
    "# --------------------------\n",
    "df = pd.read_csv(FILE_PATH, low_memory=False)\n",
    "\n",
    "# 타깃 설정\n",
    "if TARGET_COL not in df.columns:\n",
    "    sku_cols_for_target = [c for c in df.columns if re.search(r\"\\bsku\\d+\\b\", c, re.I)]\n",
    "    if not sku_cols_for_target:\n",
    "        raise ValueError(\"타깃 c_TotalProducts가 없고 SKU 관련 컬럼도 없어 타깃을 만들 수 없습니다.\")\n",
    "    df[\"__target__\"] = df[sku_cols_for_target].apply(pd.to_numeric, errors=\"coerce\").sum(axis=1)\n",
    "    TARGET = \"__target__\"\n",
    "else:\n",
    "    TARGET = TARGET_COL\n",
    "\n",
    "y_full = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "# --------------------------\n",
    "# 2) 스테이지별 대표 Queue 구성\n",
    "#    (현재 컬럼명을 최대한 포괄적으로 매칭)\n",
    "# --------------------------\n",
    "def pick_cols(patterns: List[str]) -> List[str]:\n",
    "    out = []\n",
    "    for col in df.columns:\n",
    "        lc = col.lower()\n",
    "        if \"queue\" not in lc:  # queue가 아닌 건 제외\n",
    "            continue\n",
    "        if any(re.search(p, col, re.I) for p in patterns):\n",
    "            out.append(col)\n",
    "    return out\n",
    "\n",
    "# 후보 패턴(필요시 보강)\n",
    "STAGE_PATTERNS: Dict[str, List[str]] = {\n",
    "    # Blanking: SKU별 + 요약(Blanking_Queue) 모두 포괄\n",
    "    \"blanking_sku\": [r\"^Blanking[_-]SKU[1-4]_Queue$\"],\n",
    "    \"blanking_all\": [r\"^Blanking_Queue$\"],\n",
    "    # Press\n",
    "    \"press\":        [r\"^Press\\d*_Queue$\",\"^Press\\d+_Queue$\",\"^Press_?\\d*_?Queue$\",\"^Press\"],  # 느슨하게\n",
    "    # Assembly(셀) - 있을 수도/없을 수도\n",
    "    \"assembly\":     [r\"^Cell[1-4]_Queue$\", r\"^Assembly\", r\"^c_Cell\", r\"_Assembly_Queue$\"],\n",
    "    # Warehouse (Warehouse1_Queue, Warehouse_2_Queue 등 혼재를 커버)\n",
    "    \"warehouse\":    [r\"^Warehouse\\d*_?Queue$\", r\"^Warehouse_?\\d+_?Queue$\",\"^Warehouse\"],\n",
    "    # Paint\n",
    "    \"paint\":        [r\"^Paint\\d*_Queue$\", r\"^Paint\"],\n",
    "    # Quality\n",
    "    \"quality\":      [r\"^Quality_?Queue$\",\"^Quality\"],\n",
    "    # Forklift (원하면 별도 스테이지로 보고 싶을 때)\n",
    "    \"forklift\":     [r\"^Forklift_.*_Queue$\",\"^Forklift\"]\n",
    "}\n",
    "\n",
    "# 실제 컬럼 맵핑\n",
    "stage_cols: Dict[str, List[str]] = {\n",
    "    \"blanking\": [], \"press\": [], \"assembly\": [], \"warehouse\": [], \"paint\": [], \"quality\": [], \"forklift\": []\n",
    "}\n",
    "\n",
    "# blanking은 SKU와 ALL을 구분해서 채운 뒤 전략 적용\n",
    "blanking_sku_cols = pick_cols(STAGE_PATTERNS[\"blanking_sku\"])\n",
    "blanking_all_cols = pick_cols(STAGE_PATTERNS[\"blanking_all\"])\n",
    "if BLANKING_STRATEGY == \"prefer_sku\" and blanking_sku_cols:\n",
    "    stage_cols[\"blanking\"] = blanking_sku_cols\n",
    "else:\n",
    "    stage_cols[\"blanking\"] = sorted(set(blanking_sku_cols + blanking_all_cols))\n",
    "\n",
    "# 나머지 스테이지\n",
    "for k in [\"press\",\"assembly\",\"warehouse\",\"paint\",\"quality\",\"forklift\"]:\n",
    "    stage_cols[k] = pick_cols(STAGE_PATTERNS[k])\n",
    "\n",
    "# 스테이지별 대표값 frame 만들기\n",
    "stage_rep = pd.DataFrame(index=df.index, columns=list(stage_cols.keys()), dtype=float)\n",
    "for st, cols in stage_cols.items():\n",
    "    if cols:\n",
    "        vals = df[cols].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        stage_rep[st] = vals.max(axis=1, numeric_only=True)\n",
    "    else:\n",
    "        stage_rep[st] = np.nan\n",
    "\n",
    "# z-score 비교가 필요하면 스테이지별 정규화\n",
    "if BN_SELECTION.lower() == \"zscore\":\n",
    "    zrep = stage_rep.copy()\n",
    "    for st in zrep.columns:\n",
    "        mu = zrep[st].mean(skipna=True)\n",
    "        sd = zrep[st].std(skipna=True)\n",
    "        zrep[st] = (zrep[st] - mu) / (sd if (sd and sd != 0) else np.nan)\n",
    "    comp_df = zrep\n",
    "else:\n",
    "    comp_df = stage_rep\n",
    "\n",
    "# 행별 top1 스테이지/값 (전부 NaN 또는 <=0이면 None)\n",
    "comp_filled = comp_df.fillna(-np.inf)               # 비교용(예: 여러 Queue 중 최대 찾기)\n",
    "top_stage   = comp_filled.idxmax(axis=1)            # 행별 최대가 난 컬럼명\n",
    "\n",
    "# lookup 대체: stage_rep에서 원시값(대표 Queue)을 안전하게 뽑기\n",
    "# (전제: stage_rep의 index/columns 이 comp_df와 동일하거나 superset)\n",
    "import numpy as np\n",
    "\n",
    "row_idx = np.arange(len(stage_rep))\n",
    "# top_stage(컬럼명)를 실제 열 인덱스로 변환(-1은 미존재)\n",
    "col_idx = stage_rep.columns.get_indexer(top_stage)\n",
    "\n",
    "vals = stage_rep.to_numpy().astype(float, copy=False)\n",
    "\n",
    "# col_idx == -1(해당 열이 stage_rep에 없음) 대비\n",
    "top_val = np.full(len(stage_rep), np.nan, dtype=float)\n",
    "valid = col_idx >= 0\n",
    "top_val[valid] = vals[row_idx[valid], col_idx[valid]]\n",
    "\n",
    "# 안전 처리: 모두 NaN이었던 행 또는 비유효값(<=0, 비유한) → None/NaN\n",
    "all_nan_mask = stage_rep.isna().all(axis=1)\n",
    "nonpos_mask  = (~np.isfinite(top_val)) | (top_val <= 0)\n",
    "\n",
    "bottleneck_stage = top_stage.mask(all_nan_mask | nonpos_mask, other=\"None\")\n",
    "bottleneck_value = pd.Series(top_val, index=stage_rep.index).mask(all_nan_mask | nonpos_mask, other=np.nan)\n",
    "\n",
    "# 원본 df에 기록 (index 정렬 주의: 동일 index 기준)\n",
    "df.loc[bottleneck_stage.index, \"bottleneck_stage\"] = bottleneck_stage\n",
    "df.loc[bottleneck_value.index, \"bottleneck_value\"] = bottleneck_value\n",
    "\n",
    "# 저장\n",
    "df[[\"bottleneck_stage\", \"bottleneck_value\"]].to_csv(\n",
    "    os.path.join(OUT_DIR, \"bottleneck_labels.csv\"), index=False\n",
    ")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) 병목별 그룹 통계\n",
    "# --------------------------\n",
    "stats = (\n",
    "    df.groupby(\"bottleneck_stage\")[TARGET]\n",
    "      .agg([\"mean\",\"median\",\"std\",\"count\"])\n",
    "      .reset_index()\n",
    "      .sort_values(\"mean\", ascending=False)\n",
    ")\n",
    "overall_mean = float(np.nanmean(y_full))\n",
    "stats[\"lift_vs_overall\"] = stats[\"mean\"] - overall_mean\n",
    "stats.to_csv(os.path.join(OUT_DIR, \"bottleneck_impact_summary.csv\"), index=False)\n",
    "\n",
    "# (옵션) 불량 지표 평균/중앙 병합\n",
    "defect_cols = [c for c in df.columns if re.search(r\"(defect|scrap|ng|fail|reject)\", c, re.I)]\n",
    "if defect_cols:\n",
    "    d_mean = df.groupby(\"bottleneck_stage\")[defect_cols].mean(numeric_only=True)\n",
    "    d_mean.columns = [f\"DEFECT_MEAN__{c}\" for c in d_mean.columns]\n",
    "    d_med  = df.groupby(\"bottleneck_stage\")[defect_cols].median(numeric_only=True)\n",
    "    d_med.columns  = [f\"DEFECT_MEDIAN__{c}\" for c in d_med.columns]\n",
    "    summary_def = pd.concat([stats.set_index(\"bottleneck_stage\"), d_mean, d_med], axis=1).reset_index()\n",
    "    summary_def.to_csv(os.path.join(OUT_DIR, \"bottleneck_impact_with_defects.csv\"), index=False)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10,4))\n",
    "(df[\"bottleneck_stage\"].value_counts().sort_values(ascending=False)).plot(kind=\"bar\")\n",
    "plt.title(\"Bottleneck Frequency (by Stage)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"plot_bottleneck_frequency.png\"), dpi=150); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(stats[\"bottleneck_stage\"], stats[\"mean\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Mean Throughput\")\n",
    "plt.title(\"Mean Throughput by Bottleneck Stage\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"plot_mean_throughput_by_bottleneck.png\"), dpi=150); plt.close()\n",
    "\n",
    "# --------------------------\n",
    "# 4) SKU별 리드타임 컬럼 수집\n",
    "#    (SKU1~4 × {VA,NVA,Transport,Wait,Other}_Time)\n",
    "# --------------------------\n",
    "SKUS = [1,2,3,4]\n",
    "TIME_KINDS = [\"VA\",\"NVA\",\"Transport\",\"Wait\",\"Other\"]\n",
    "\n",
    "def find_leadtime_cols(df_cols) -> Dict[Tuple[int,str], List[str]]:\n",
    "    \"\"\"(sku, kind) -> 매칭된 리드타임 컬럼 목록\"\"\"\n",
    "    mapping = {}\n",
    "    for sku in SKUS:\n",
    "        for kind in TIME_KINDS:\n",
    "            # 예: \"SKU1_VA_Time\", \"SKU1-VA Time\", \"sku1 va time\" 등 다양한 표기 허용\n",
    "            pat = re.compile(rf\"\\bSKU[_\\- ]?{sku}\\b.*\\b{kind}\\b.*\\bTime\\b\", re.I)\n",
    "            cols = [c for c in df_cols if pat.search(c)]\n",
    "            mapping[(sku, kind)] = cols\n",
    "    return mapping\n",
    "\n",
    "lead_cols_map = find_leadtime_cols(df.columns)\n",
    "lead_cols_flat = sorted({c for cols in lead_cols_map.values() for c in cols})\n",
    "lead_df = df[lead_cols_flat].apply(pd.to_numeric, errors=\"coerce\") if lead_cols_flat else pd.DataFrame(index=df.index)\n",
    "\n",
    "# 보조 파생: SKU별 총 Time, 비율\n",
    "ratio_cols = []\n",
    "if not lead_df.empty:\n",
    "    for sku in SKUS:\n",
    "        per_sku_cols = {kind: lead_cols_map[(sku, kind)] for kind in TIME_KINDS}\n",
    "        # kind별 합(여러 컬럼이 잡힌 경우 합산)\n",
    "        sums = {}\n",
    "        for kind, cols in per_sku_cols.items():\n",
    "            if cols:\n",
    "                sums[kind] = df[cols].apply(pd.to_numeric, errors=\"coerce\").sum(axis=1)\n",
    "            else:\n",
    "                sums[kind] = pd.Series(np.nan, index=df.index)\n",
    "        total = sum(sums.values())\n",
    "        df[f\"SKU{sku}_Total_Time\"] = total\n",
    "        # 비율 컬럼 추가\n",
    "        for kind in TIME_KINDS:\n",
    "            df[f\"SKU{sku}_{kind}_Ratio\"] = sums[kind] / total.replace({0: np.nan})\n",
    "            ratio_cols.append(f\"SKU{sku}_{kind}_Ratio\")\n",
    "\n",
    "# --------------------------\n",
    "# 5) 모델 입력 구축\n",
    "#    - 병목 스테이지 원핫 (BN_)\n",
    "#    - 리드타임 원본 + 파생(총/비율) 포함\n",
    "# --------------------------\n",
    "# 병목 스테이지 원핫\n",
    "bn_label = df[\"bottleneck_stage\"].fillna(\"None\").astype(str).to_frame()\n",
    "try:\n",
    "    ohe_bn = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe_bn = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "bn_ohe_mat = ohe_bn.fit_transform(bn_label)\n",
    "bn_ohe_cols = [f\"BN_{c}\" for c in ohe_bn.categories_[0]]\n",
    "X_bn = pd.DataFrame(bn_ohe_mat, columns=bn_ohe_cols, index=df.index)\n",
    "\n",
    "# 리드타임 피처(원본 + 파생)\n",
    "X_lt_parts = []\n",
    "if not lead_df.empty:\n",
    "    X_lt_parts.append(lead_df)              # 원본 리드타임\n",
    "if ratio_cols:\n",
    "    X_lt_parts.append(df[ratio_cols])       # 비율\n",
    "total_cols = [c for c in df.columns if re.fullmatch(r\"SKU[1-4]_Total_Time\", c)]\n",
    "if total_cols:\n",
    "    X_lt_parts.append(df[total_cols])       # 총합\n",
    "X_lt = pd.concat(X_lt_parts, axis=1) if X_lt_parts else pd.DataFrame(index=df.index)\n",
    "\n",
    "# 최종 설계행렬 2종: (A) 병목만, (B) 병목+리드타임\n",
    "mask = ~y_full.isna()\n",
    "y = y_full.loc[mask].reset_index(drop=True)\n",
    "X_A = X_bn.loc[mask].reset_index(drop=True)\n",
    "X_B = pd.concat([X_bn, X_lt], axis=1).loc[mask].reset_index(drop=True)\n",
    "\n",
    "# 전처리 파이프 (수치만 존재하면 간단히 스케일만)\n",
    "def make_preprocess(X: pd.DataFrame):\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "    num_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "    if cat_cols:\n",
    "        try:\n",
    "            cat_ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        except TypeError:\n",
    "            cat_ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "        cat_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", cat_ohe)])\n",
    "        prep = ColumnTransformer([(\"num\", num_tf, num_cols), (\"cat\", cat_tf, cat_cols)], remainder=\"drop\")\n",
    "    else:\n",
    "        prep = ColumnTransformer([(\"num\", num_tf, num_cols)], remainder=\"drop\")\n",
    "    return prep\n",
    "\n",
    "pre_A = make_preprocess(X_A)\n",
    "pre_B = make_preprocess(X_B)\n",
    "\n",
    "# --------------------------\n",
    "# 6) 회귀: A(병목만) vs B(병목+리드타임)\n",
    "# --------------------------\n",
    "def eval_regression(X: pd.DataFrame, y: pd.Series, tag: str):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "    pre = make_preprocess(X)\n",
    "\n",
    "    ridge = Pipeline([(\"prep\", pre), (\"model\", RidgeCV(alphas=np.logspace(-3,3,20)))])\n",
    "    rf    = Pipeline([(\"prep\", pre), (\"model\", RandomForestRegressor(n_estimators=400, random_state=RANDOM_SEED, n_jobs=-1))])\n",
    "\n",
    "    results = {}\n",
    "    for name, pipe in [(\"RidgeCV\", ridge), (\"RandomForest\", rf)]:\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        pred = pipe.predict(X_te)\n",
    "        out = {\n",
    "            \"MAE\": float(mean_absolute_error(y_te, pred)),\n",
    "            \"RMSE\": float(np.sqrt(mean_squared_error(y_te, pred))),\n",
    "            \"R2\": float(r2_score(y_te, pred))\n",
    "        }\n",
    "        results[name] = out\n",
    "        # 저장\n",
    "        joblib.dump(pipe, os.path.join(OUT_DIR, f\"model_{name}_{tag}.pkl\"))\n",
    "        pd.DataFrame({\"y_true\": y_te, \"y_pred\": pred}).to_csv(\n",
    "            os.path.join(OUT_DIR, f\"predictions_{name}_{tag}.csv\"), index=False\n",
    "        )\n",
    "    # 메트릭 저장\n",
    "    with open(os.path.join(OUT_DIR, f\"regression_metrics_{tag}.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    return results, (X_tr, X_te, y_tr, y_te), pre\n",
    "\n",
    "res_A, split_A, preA = eval_regression(X_A, y, \"BNonly\")\n",
    "res_B, split_B, preB = eval_regression(X_B, y, \"BNplusLT\")\n",
    "\n",
    "print(\"[REG] A: BN only =>\", res_A)\n",
    "print(\"[REG] B: BN + LeadTime =>\", res_B)\n",
    "\n",
    "# --------------------------\n",
    "# 7) 중요도/영향도: 리드타임 중심\n",
    "#    - RandomForest permutation importance 대체: 트리 feature_importances_\n",
    "#    - Ridge 계수(표준화 후) 절대값\n",
    "# --------------------------\n",
    "def export_feature_lists(pre: ColumnTransformer, X_cols: List[str]) -> List[str]:\n",
    "    \"\"\"ColumnTransformer를 통과한 최종 feature 이름을 최대한 추정\"\"\"\n",
    "    names = []\n",
    "    for name_, trans, cols in pre.transformers_:\n",
    "        if name_ == \"remainder\" and trans == \"drop\":\n",
    "            continue\n",
    "        if hasattr(trans, \"named_steps\"):\n",
    "            # 수치 파이프\n",
    "            if \"imp\" in trans.named_steps and hasattr(trans.named_steps[\"imp\"], \"get_feature_names_out\"):\n",
    "                try:\n",
    "                    names += list(trans.named_steps[\"imp\"].get_feature_names_out(cols))\n",
    "                    continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # 범주 + OHE\n",
    "            if \"ohe\" in trans.named_steps and hasattr(trans.named_steps[\"ohe\"], \"get_feature_names_out\"):\n",
    "                try:\n",
    "                    names += list(trans.named_steps[\"ohe\"].get_feature_names_out(cols))\n",
    "                    continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # fallback\n",
    "        if isinstance(cols, list):\n",
    "            names += list(cols)\n",
    "        else:\n",
    "            names += [cols]\n",
    "    return names\n",
    "\n",
    "def rf_importance(pipe: Pipeline, out_csv: str, top_png: str, top_k:int=TOP_K_IMP):\n",
    "    rf = pipe.named_steps.get(\"model\")\n",
    "    pre = pipe.named_steps.get(\"prep\")\n",
    "    if not hasattr(rf, \"feature_importances_\"):\n",
    "        return None\n",
    "    feat_names = export_feature_lists(pre, [])\n",
    "    imp = pd.Series(rf.feature_importances_, index=feat_names).sort_values(ascending=False)\n",
    "    imp.to_csv(out_csv, header=[\"importance\"])\n",
    "    # 그래프\n",
    "    top = imp.head(top_k)\n",
    "    plt.figure(figsize=(8, max(3, 0.4*len(top))))\n",
    "    plt.barh(top.index[::-1], top.values[::-1])\n",
    "    plt.title(\"RandomForest Feature Importances (Top {})\".format(top_k))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(top_png, dpi=150); plt.close()\n",
    "    return imp\n",
    "\n",
    "def ridge_coeff(pipe: Pipeline, out_csv: str):\n",
    "    # 표준화 이후 계수 → 절대값이 클수록 영향 큼\n",
    "    model = pipe.named_steps.get(\"model\")\n",
    "    pre   = pipe.named_steps.get(\"prep\")\n",
    "    if not hasattr(model, \"coef_\"):\n",
    "        return None\n",
    "    feat_names = export_feature_lists(pre, [])\n",
    "    coefs = pd.Series(np.ravel(model.coef_), index=feat_names)\n",
    "    coefs.to_csv(out_csv, header=[\"coef\"])\n",
    "    return coefs\n",
    "\n",
    "# BN+LT 모델 중요도 내보내기\n",
    "ridgeB = joblib.load(os.path.join(OUT_DIR, \"model_RidgeCV_BNplusLT.pkl\"))\n",
    "rfB    = joblib.load(os.path.join(OUT_DIR, \"model_RandomForest_BNplusLT.pkl\"))\n",
    "\n",
    "imp_rf_B = rf_importance(\n",
    "    rfB,\n",
    "    os.path.join(OUT_DIR, \"feature_importance_RF_BNplusLT.csv\"),\n",
    "    os.path.join(OUT_DIR, \"plot_feature_importance_RF_BNplusLT_top{}.png\".format(TOP_K_IMP)),\n",
    "    top_k=TOP_K_IMP\n",
    ")\n",
    "coef_ridge_B = ridge_coeff(\n",
    "    ridgeB,\n",
    "    os.path.join(OUT_DIR, \"ridge_coeff_BNplusLT.csv\")\n",
    ")\n",
    "\n",
    "# 리드타임 관련 항목만 필터링해서 별도 저장/힛맵\n",
    "def is_lead_feature(name: str) -> bool:\n",
    "    return bool(re.search(r\"(SKU[1-4].*(VA|NVA|Transport|Wait|Other).*Time)|(_Ratio$)|(_Total_Time$)\", name, re.I))\n",
    "\n",
    "if imp_rf_B is not None:\n",
    "    imp_lt = imp_rf_B[imp_rf_B.index.map(is_lead_feature)]\n",
    "    imp_lt.to_csv(os.path.join(OUT_DIR, \"feature_importance_RF__LeadTime_only.csv\"), header=[\"importance\"])\n",
    "\n",
    "    # SKU × Kind 피벗(합산 중요도)\n",
    "    rows = []\n",
    "    for feat, val in imp_lt.items():\n",
    "        m = re.search(r\"(SKU[1-4]).*(VA|NVA|Transport|Wait|Other)\", feat, re.I)\n",
    "        if m:\n",
    "            sku = m.group(1).upper()\n",
    "            kind = m.group(2).capitalize()\n",
    "            rows.append((sku, kind, float(val)))\n",
    "    if rows:\n",
    "        lt_imp_df = pd.DataFrame(rows, columns=[\"SKU\",\"Kind\",\"importance\"])\n",
    "        pivot = lt_imp_df.pivot_table(index=\"SKU\", columns=\"Kind\", values=\"importance\", aggfunc=\"sum\").fillna(0.0)\n",
    "        pivot.to_csv(os.path.join(OUT_DIR, \"leadtime_importance_pivot_RF.csv\"))\n",
    "\n",
    "        # 히트맵(단색 막대 대체)\n",
    "        plt.figure(figsize=(7,4))\n",
    "        # 간단 heatmap 구현 (matplotlib)\n",
    "        im = plt.imshow(pivot.values, aspect=\"auto\")\n",
    "        plt.xticks(ticks=np.arange(len(pivot.columns)), labels=pivot.columns, rotation=45, ha=\"right\")\n",
    "        plt.yticks(ticks=np.arange(len(pivot.index)), labels=pivot.index)\n",
    "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "        plt.title(\"Lead-time Importance (RF, summed)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"plot_leadtime_importance_RF_heatmap.png\"), dpi=150); plt.close()\n",
    "\n",
    "# 상관 분석(보조 인사이트)\n",
    "if not X_lt.empty:\n",
    "    corr_rows = []\n",
    "    for c in X_lt.columns:\n",
    "        x = pd.to_numeric(X_lt[c], errors=\"coerce\")\n",
    "        m = (~x.isna()) & (~y_full.isna())\n",
    "        if m.sum() > 3:\n",
    "            r = np.corrcoef(x[m], y_full[m])[0,1]\n",
    "        else:\n",
    "            r = np.nan\n",
    "        corr_rows.append({\"feature\": c, \"pearson_r\": r})\n",
    "    pd.DataFrame(corr_rows).sort_values(\"pearson_r\", ascending=False)\\\n",
    "        .to_csv(os.path.join(OUT_DIR, \"leadtime_vs_throughput_corr.csv\"), index=False)\n",
    "\n",
    "# --------------------------\n",
    "# 8) (옵션) 분류: 상위 10% 생산량 여부\n",
    "# --------------------------\n",
    "q90 = float(np.nanpercentile(y, 90))\n",
    "y_bin = (y >= q90).astype(int)\n",
    "X_trB, X_teB, y_trB, y_teB = train_test_split(X_B, y_bin, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "pre_cls = make_preprocess(X_B)\n",
    "logreg = Pipeline([(\"prep\", pre_cls), (\"clf\", LogisticRegression(max_iter=300, class_weight=\"balanced\"))])\n",
    "logreg.fit(X_trB, y_trB)\n",
    "proba = logreg.predict_proba(X_teB)[:,1]\n",
    "auc = roc_auc_score(y_teB, proba)\n",
    "report = classification_report(y_teB, (proba>=0.5).astype(int), output_dict=True)\n",
    "with open(os.path.join(OUT_DIR, \"classification_report_top10pct_BNplusLT.json\"), \"w\") as f:\n",
    "    json.dump({\"ROC_AUC\": auc, \"report\": report}, f, indent=2)\n",
    "print(f\"[CLS] BN+LT Top10% ROC-AUC={auc:.3f}\")\n",
    "\n",
    "# --------------------------\n",
    "# 9) 저장물 안내\n",
    "# --------------------------\n",
    "print(\"\\n[Saved files @ {}]\".format(OUT_DIR))\n",
    "print(\" - bottleneck_labels.csv\")\n",
    "print(\" - bottleneck_impact_summary.csv\", \"(+ bottleneck_impact_with_defects.csv if available)\")\n",
    "print(\" - plot_bottleneck_frequency.png\")\n",
    "print(\" - plot_mean_throughput_by_bottleneck.png\")\n",
    "print(\" - regression_metrics_BNonly.json (Ridge/RF)\")\n",
    "print(\" - regression_metrics_BNplusLT.json (Ridge/RF)\")\n",
    "print(\" - model_RidgeCV_BNonly.pkl / predictions_RidgeCV_BNonly.csv\")\n",
    "print(\" - model_RandomForest_BNonly.pkl / predictions_RandomForest_BNonly.csv\")\n",
    "print(\" - model_RidgeCV_BNplusLT.pkl / predictions_RidgeCV_BNplusLT.csv\")\n",
    "print(\" - model_RandomForest_BNplusLT.pkl / predictions_RandomForest_BNplusLT.csv\")\n",
    "print(\" - feature_importance_RF_BNplusLT.csv\")\n",
    "print(\" - plot_feature_importance_RF_BNplusLT_top{}.png\".format(TOP_K_IMP))\n",
    "print(\" - ridge_coeff_BNplusLT.csv\")\n",
    "print(\" - feature_importance_RF__LeadTime_only.csv (if any)\")\n",
    "print(\" - leadtime_importance_pivot_RF.csv / plot_leadtime_importance_RF_heatmap.png (if any)\")\n",
    "print(\" - leadtime_vs_throughput_corr.csv (if any)\")\n",
    "print(\" - classification_report_top10pct_BNplusLT.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b70c44-5852-4197-a36f-d69e2ab2d46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b611937-420b-4fe5-bf72-9b2589607384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114684f-67a3-4815-affe-faa1cd981058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ecd6f-df56-4de1-97b3-1e1555e5b1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
