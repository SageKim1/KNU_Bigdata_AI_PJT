import torch

print("ğŸ§  PyTorch ë²„ì „:", torch.__version__)
print("ğŸ§ª PyTorch ë¹Œë“œ ì‹œ ì‚¬ìš©í•œ CUDA ë²„ì „:", torch.version.cuda)

if torch.cuda.is_available():
    device = torch.device("cuda:0")
    props = torch.cuda.get_device_properties(device)

    print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ (GPU ì‚¬ìš© ì¤‘)")
    print("ğŸ”§ GPU ì´ë¦„:", props.name)
    print("ğŸ”¢ Compute Capability:", props.major, ".", props.minor)
    print("ğŸ§® ì´ ë©”ëª¨ë¦¬:", f"{props.total_memory / 1024**2:.2f} MB")

    print("ğŸ“¦ í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬:", f"{torch.cuda.memory_allocated(device) / 1024**2:.2f} MB")
    print("ğŸ“¦ í˜„ì¬ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬:", f"{torch.cuda.memory_reserved(device) / 1024**2:.2f} MB")
    print("ğŸ“¦ ìµœëŒ€ í• ë‹¹ ë©”ëª¨ë¦¬ (ì´ ì„¸ì…˜ ê¸°ì¤€):", f"{torch.cuda.max_memory_allocated(device) / 1024**2:.2f} MB")

    print("\nğŸ“‹ ë©”ëª¨ë¦¬ ìƒíƒœ ìš”ì•½:")
    print(torch.cuda.memory_summary(device=device, abbreviated=True))

else:
    print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€. í˜„ì¬ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
